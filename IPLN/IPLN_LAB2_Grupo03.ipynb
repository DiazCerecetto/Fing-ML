{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4azOYi8KSoC"
      },
      "source": [
        "# Natural Language Processing Lab\n",
        "\n",
        "This work was made on spanish for \"FIng - UDELAR, Montevideo Uruguay\" as a practice work for \"Introducción al Procesamiento del Lenguaje Natural\" (Introduction to NLP).\n",
        "\n",
        "# Task 2\n",
        "\n",
        "The objective of this task is to carry out various experiments to represent and classify texts. For this purpose, we will work with a corpus for sentiment analysis, created for the [TASS 2020](http://www.sepln.org/workshops/tass/) competition (IberLEF - SEPLN).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAumcYFLP0f8"
      },
      "source": [
        "# Parte 1 - Carga y preprocesamiento del corpus\n",
        "\n",
        "Para trabajar en este notebook deben cargar los tres archivos disponbiles en eva: train.csv, devel.csv y test.csv.\n",
        "\n",
        "La aplicación de una etapa de preprocesamiento similar a la implementada en la tarea 1 es opcional. Es interesante hacer experimentos con y sin la etapa de preprocesamiento, de modo de comparar resultados (sobre el corpus de desarrollo, devel.csv) y definir si se incluye o no en la solución final.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xpi3LU-qwDP"
      },
      "outputs": [],
      "source": [
        "#Imports necesarios\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')           #Para Stop words\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "nnBJGtH5QLcA",
        "outputId": "87d35a3a-f2fe-4545-93e5-80a8cbcc8da1"
      },
      "outputs": [],
      "source": [
        "# Carga de los datasets\n",
        "\n",
        "with open('train.csv', newline='', encoding=\"utf-8\") as corpus_csv:\n",
        "  reader = csv.reader(corpus_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  train_set = [x for x in reader]\n",
        "  train_set_lexicos = [x for x in reader]\n",
        "\n",
        "with open('devel.csv', newline='', encoding=\"utf-8\") as corpus_csv2:\n",
        "  reader2 = csv.reader(corpus_csv2)\n",
        "  next(reader2) # Saltea el cabezal del archivo\n",
        "  devel_set = [x for x in reader2]\n",
        "\n",
        "with open('test.csv', newline='', encoding=\"utf-8\") as corpus_csv3:\n",
        "  reader3 = csv.reader(corpus_csv3)\n",
        "  next(reader3) # Saltea el cabezal del archivo\n",
        "  test_set = [x for x in reader3]\n",
        "\n",
        "with open('lexico_pos_lemas_grande.csv', newline='', encoding=\"utf-8\") as corpus_csv4:\n",
        "  reader4 = csv.reader(corpus_csv4)\n",
        "  next(reader4) # Saltea el cabezal del archivo\n",
        "  pos_set = [x for x in reader4]\n",
        "\n",
        "with open('lexico_neg_lemas_grande.csv', newline='', encoding=\"utf-8\") as corpus_csv5:\n",
        "  reader5 = csv.reader(corpus_csv5)\n",
        "  next(reader5) # Saltea el cabezal del archivo\n",
        "  neg_set = [x for x in reader5]\n",
        "\n",
        "#Cargamos Stopwords\n",
        "with open('stop_words_esp_anasent.csv', newline='', encoding=\"utf-8\") as stop_words_csv:\n",
        "  reader = csv.reader(stop_words_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  stop_words_set = [x[0] for x in reader]\n",
        "\n",
        "\n",
        "#Definimos el corpus train_lexicos siendo este train y los léxicos proporcionados agregados como tweets:\n",
        "for elem in neg_set:\n",
        "  train_set_lexicos.insert(0,[0,elem[0],'N'])\n",
        "\n",
        "for elem in pos_set:\n",
        "  train_set_lexicos.insert(0,[0,elem[0],'P'])\n",
        "\n",
        "\n",
        "# Elegir un tweet aleatorio para el corpus train e imprimirlo junto a su categoría\n",
        "random_tweet = random.choice(train_set)\n",
        "print(f\"El tweet tiene id: {random_tweet[0]}\")\n",
        "print(f\"El tweet es: {random_tweet[1]}\")\n",
        "print(f\"y su categoría: {random_tweet[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YyPwXDorQCT"
      },
      "source": [
        "Como se puede observar, al Corpus de entrenamiento decidimos agregarle en un nuevo Data Set: train_set léxicos, los lemas positivos y negativos brindados via EVA, con la correspondiente polaridad. A su vez crearemos un Data Set similar donde combinaremos palabras de los léxicos positivos y negativos, asignando la polaridad \"NONE\" a dichas combinaciones. Dicho Data Set sera: train_modificado, y sera utilizado para entrenar distintos modelos de clasificación impemnatdos en la Parte 3. Para intentar que train_modificado quede lo mas balanceado posible, primero determinamos con cuantos léxicos positivos y negativos contamos, y la proporción de las distintas polaridades en el Data Set de entrenamiento original:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jloqIW6lfRqt",
        "outputId": "bf540b87-f06a-4516-ec84-e403822db74c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de palabras que contiene el lexico positivo:  3354\n",
            "Cantidad de palabras que contiene el lexico negativo:  4796\n",
            "Cantidad de tweets positivos en train  2967\n",
            "Cantidad de tweets negativos en train  2639\n",
            "Cantidad de tweets neutros en train  2708\n"
          ]
        }
      ],
      "source": [
        "print(\"Cantidad de palabras que contiene el lexico positivo: \",len(pos_set))\n",
        "print(\"Cantidad de palabras que contiene el lexico negativo: \",len(neg_set))\n",
        "pos_train = 0\n",
        "neg_train = 0\n",
        "none_train = 0\n",
        "for t in train_set:\n",
        "  if t[2] ==\"P\":\n",
        "    pos_train +=1\n",
        "  elif t[2] ==\"N\":\n",
        "    neg_train +=1\n",
        "  else:\n",
        "    none_train +=1\n",
        "\n",
        "print(\"Cantidad de tweets positivos en train \",pos_train)\n",
        "print(\"Cantidad de tweets negativos en train \",neg_train)\n",
        "print(\"Cantidad de tweets neutros en train \",none_train) #suma = 8.314"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S--eZw4Fg29z"
      },
      "source": [
        "En función de esto, agregaremos a train_modifiado, 3354 \"tweets\" positivos, 3354 \"tweets\" negativos y 3354 \"tweets\" neutros:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LumPZvgZhFTu",
        "outputId": "eb26034f-beb2-4605-8539-f5f2e3ad4382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de tweets en train_modificado:  18373\n"
          ]
        }
      ],
      "source": [
        "train_modificado = train_set.copy()\n",
        "for elem in range(0,3353):\n",
        "  auxP = pos_set[elem]\n",
        "  auxN = neg_set[elem]\n",
        "  auxNone = auxP[0] + \" \" + auxN[0]\n",
        "  train_modificado.insert(0,[0,auxNone,'NONE'])\n",
        "  train_modificado.insert(0,[0,auxP[0],'P'])\n",
        "  train_modificado.insert(0,[0,auxN[0],'N'])\n",
        "print(\"Cantidad de tweets en train_modificado: \",len(train_modificado))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKIZ96949tlh"
      },
      "source": [
        "## Preprocesamiento de tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2kOeMA_ly_b"
      },
      "outputs": [],
      "source": [
        "# Preprocesamiento de los tweets\n",
        "\n",
        "#############################################################################################\n",
        "###########################  Código de preprocesamiento del LAB 1  ##########################\n",
        "#############################################################################################\n",
        "\n",
        "# reemplazar_URL(texto):\n",
        "#Reemplaza las URL del texto recibido por parámetro por el string vacío.\n",
        "#Retorna además un identificador para controlar si hubo algún cambio en el tweet\n",
        "#analizado, útil para realizar recorridas de testeo\n",
        "def reemplazar_URL(texto):\n",
        "    texto2 = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"\", texto)\n",
        "    cambio = (texto2!=texto)\n",
        "    return [texto2,cambio]\n",
        "\n",
        "\n",
        "\n",
        "# reemplazar_Usuario(texto):\n",
        "#Reemplaza las menciones a usuarios del texto recibido por parámetro por el string vacío.\n",
        "#Retorna además un identificador para controlar si hubo algún cambio en el tweet\n",
        "#analizado, útil para realizar recorridas de testeo\n",
        "def reemplazar_Usuario(texto):\n",
        "    regex2 = r\"@(\\w+)\"\n",
        "    texto2 = re.sub(regex2,\"\", texto)\n",
        "    cambio = (texto2!=texto)\n",
        "    return [texto2,cambio]\n",
        "\n",
        "\n",
        "\n",
        "# reemplazar_Abreviaturas(texto):\n",
        "#Remplazar abreviaturas comunes por el término original.\n",
        "#Retorna además un identificador para controlar si hubo algún cambio en el tweet\n",
        "#analizado, útil para realizar recorridas de testeo\n",
        "def reemplazar_Abreviaturas(texto):\n",
        "    texto2 = re.sub(r'[\\s^][xX][qQ][\\s$]',' porque ', texto)\n",
        "    texto2 = re.sub(r'[\\s^][pP](\\s)*[qQ][\\s$]',' porque ', texto2)\n",
        "    texto2 = re.sub(r'porq',' porque ', texto2)\n",
        "    texto2 = re.sub('[\\s^][xX][\\s$]',' por ', texto2)\n",
        "    texto2 = re.sub('[\\s^][qQ][\\s$]',' que ', texto2)\n",
        "    texto2 = re.sub('[\\s^][kK][\\s$]',' que ', texto2)\n",
        "    texto2 = re.sub('[\\s^][bB][nN][\\s$]',' bien ', texto2)\n",
        "    texto2 = re.sub('[\\s^][tT][mM][bB][\\s$]',' tambien ', texto2)\n",
        "    # Consideramos que no corresponde reemplazar RT\n",
        "    # como una abreviatura ya que puede cambiar el significado semántico de la oración,\n",
        "    # y lo borramos directamente.\n",
        "    texto2 = re.sub('[\\s^][rR][tT][\\s$]',' ', texto2)\n",
        "    texto2 = re.sub('[\\s^][aA][cC][eE][Ss][\\s$]',' haces ', texto2)\n",
        "    texto2 = re.sub('[\\s^][bB][bB][\\?*\\s$]',' bebé ', texto2)\n",
        "    texto2 = re.sub('[\\s^][bB][bB][sS][\\?*\\s$]',' bebés ', texto2)\n",
        "    texto2 = re.sub('[\\s^][vV][sS][\\s$]',' versus ', texto2)\n",
        "    texto2 = re.sub('[\\s^][cC][\\s$]',' se ', texto2)\n",
        "    texto2 = re.sub('[\\s^]\\+[\\s$]',' mas ', texto2)\n",
        "    texto2 = re.sub('[\\s^][dD][\\s$]',' de ', texto2)\n",
        "    texto2 = re.sub('[\\s^][dD][lL][\\s$]',' del ', texto2)\n",
        "    texto2 = re.sub('[\\s^][tT][aA][\\s$]',' está ', texto2)\n",
        "    texto2 = re.sub('[\\s^][pP][aA][\\s$]',' para ', texto2)\n",
        "    texto2 = re.sub('[\\s^][pP][sS][\\?*\\.*\\,*\\s$]',' pues ', texto2)\n",
        "    texto2 = re.sub('[\\s^][mM][\\s$]',' me ', texto2)\n",
        "    texto2 = re.sub('[\\s^][cC][sS][mM][\\s$]',' insulto ', texto2)  #Cambiamos csm por insulto.\n",
        "    texto2 = re.sub('[\\s^][gG]ral[\\s.$]',' general ', texto2)\n",
        "    texto2 = re.sub('[\\s^][dD][rR][.\\s$]',' doctor ', texto2)\n",
        "    texto2 = re.sub('[\\s^][mM][gG][\\s$]',' me gusta ', texto2)\n",
        "    cambio = (texto2!=texto)\n",
        "    return [texto2,cambio]\n",
        "\n",
        "\n",
        "\n",
        "# reemplazar_Risa(texto):\n",
        "#Remplazar en este caso las variantes posibles de una risa (expresion frecuente) por el String \"jajaja\".\n",
        "#Retorna además un identificador para controlar si hubo algún cambio en el tweet\n",
        "#analizado, útil para realizar recorridas de testeo\n",
        "def reemplazar_Risa(texto):\n",
        "    er_risa = r'\\b([aA]+[jJ]+[aA]+[aAjJ]*|[jJ]+[jaJA]+[jJ]+[jaJA]*|[aA]+[hH]+[aA]+[aAhH]*|[hH]+[haHA]+[hH]+[haHA]*|[oO]?[lL]+[oO]+[lL]+[oLOl]*|[aA]*[jaJA]+[jJ][jJAa]*|[eE]+[jJ]+[eE]+[eEjJ]*|[jJ]+[jeJE]+[jJ]+[jeJE]*|[eE]+[hH]+[eE]+[eEhH]*|[hH]+[heHE]+[hH]+[heHE]*|[eE]*[jeJE]+[jJ][jJeE]*)\\b'\n",
        "    texto2 = re.sub(er_risa,' jajaja ', texto)\n",
        "    cambio = (texto2!=texto)\n",
        "    return [texto2,cambio]\n",
        "\n",
        "\n",
        "\n",
        "# strip_accents(texto)\n",
        "#Remueve los acentos del texto\n",
        "def strip_accents(texto):\n",
        "    texto2 = re.sub(\"á\",\"a\",texto)\n",
        "    texto2 = re.sub(\"é\",\"e\",texto2)\n",
        "    texto2 = re.sub(\"í\",\"i\",texto2)\n",
        "    texto2 = re.sub(\"ó\",\"o\",texto2)\n",
        "    texto2 = re.sub(\"ú\",\"u\",texto2)\n",
        "    return texto2\n",
        "\n",
        "\n",
        "\n",
        "# reemplazar_Hashtags\n",
        "# reemplazamos hashtags del texto por el string \"HASHTAG\".\n",
        "# Retorna además un identificador para controlar si hubo algún cambio en el tweet\n",
        "# analizado, útil para realizar recorridas de testeo\n",
        "def reemplazar_Hashtags(texto):\n",
        "    er_hashtags = r'#'                      #Nos quedamos con lo que le seguia por si acaso.\n",
        "    texto2 = re.sub(er_hashtags,'', texto)\n",
        "    cambio = (texto2!=texto)\n",
        "    return [texto2,cambio]\n",
        "\n",
        "\n",
        "\n",
        "# reemplazar_repeticiones(texto)\n",
        "# Reemplaza las repeticiones de 3 o más veces la misma letra por una sola en \"texto\"\n",
        "def reemplazar_repeticiones(texto):\n",
        "    texto = re.sub(\"aaa[a]*\",\"a\",texto)\n",
        "    texto = re.sub(\"bbb[b]*\",\"b\",texto)\n",
        "    texto = re.sub(\"ccc[c]*\",\"c\",texto)\n",
        "    texto = re.sub(\"ddd[a]*\",\"d\",texto)\n",
        "    texto = re.sub(\"eee[e]*\",\"e\",texto)\n",
        "    texto = re.sub(\"fff[f]*\",\"f\",texto)\n",
        "    texto = re.sub(\"ggg[g]*\",\"g\",texto)\n",
        "    texto = re.sub(\"hhh[h]*\",\"h\",texto)\n",
        "    texto = re.sub(\"iii[i]*\",\"i\",texto)\n",
        "    texto = re.sub(\"jjj[j]*\",\"j\",texto)\n",
        "    texto = re.sub(\"kkk[k]*\",\"k\",texto)\n",
        "    texto = re.sub(\"lll[l]*\",\"l\",texto)\n",
        "    texto = re.sub(\"mmm[m]*\",\"m\",texto)\n",
        "    texto = re.sub(\"nnn[n]*\",\"n\",texto)\n",
        "    texto = re.sub(\"ooo[o]*\",\"o\",texto)\n",
        "    texto = re.sub(\"ppp[p]*\",\"p\",texto)\n",
        "    texto = re.sub(\"qqq[q]*\",\"q\",texto)\n",
        "    texto = re.sub(\"rrr[r]*\",\"r\",texto)\n",
        "    texto = re.sub(\"sss[s]*\",\"s\",texto)\n",
        "    texto = re.sub(\"ttt[t]*\",\"t\",texto)\n",
        "    texto = re.sub(\"uuu[u]*\",\"u\",texto)\n",
        "    texto = re.sub(\"vvv[v]*\",\"v\",texto)\n",
        "    texto = re.sub(\"www[w]*\",\"w\",texto)  #Luego de Urls\n",
        "    texto = re.sub(\"xxx[x]*\",\"x\",texto)\n",
        "    texto = re.sub(\"yyy[y]*\",\"y\",texto)\n",
        "    texto = re.sub(\"zzz[z]*\",\"z\",texto)\n",
        "    return texto\n",
        "\n",
        "\n",
        "\n",
        "# reemplazar_Groserias(texto)\n",
        "# Reemplaza las groserías de \"texto\" por el string \"GROSERIA\"\n",
        "# Retorna además un identificador para controlar si hubo algún cambio en el tweet\n",
        "# analizado, útil para realizar recorridas de testeo\n",
        "def reemplazar_Groserias(texto):\n",
        "    regex1 = r'hij[oa]+[de]*[p]+[u]+[t]+[a]+|gilipollas|gilipolleces|bolud[oa]|mierda|mariconadas|estupid[oa]|estupide(z|ces)'\n",
        "    regex2 = r'\\sput[oa]\\s|marica\\s'\n",
        "    texto2 = re.sub(regex1,' insulto ', texto)\n",
        "    texto2 = re.sub(regex2,' insulto ', texto2)\n",
        "    cambio = (texto2!=texto)\n",
        "    return [texto2,cambio]\n",
        "\n",
        "\n",
        "\n",
        "#Removemos 2 o mas espacios seguidos\n",
        "def remover_espacios(texto):\n",
        "  patron_espacios = r'\\s+'\n",
        "  texto = re.sub(patron_espacios, \" \", texto)\n",
        "  return texto\n",
        "\n",
        "def remove_stop_words(texto):\n",
        "  tokens_texto = word_tokenize(texto)\n",
        "  sin_stop = [word for word in tokens_texto if not word in stop_words_set]\n",
        "  texto_filtrado = (\" \").join(sin_stop)\n",
        "  return texto_filtrado\n",
        "\n",
        "def alfanumericos(text):\n",
        "    text=re.sub(r'[^a-zA-Z\\s]',' ',text)\n",
        "    return text\n",
        "\n",
        "def remover_letras(texto):\n",
        "  texto_limpio = re.sub(r'\\b\\w\\b', '', texto)\n",
        "  return texto_limpio\n",
        "\n",
        "def eliminar_url(texto):\n",
        "    patron_url = r'http'\n",
        "    texto_limpio = re.sub(patron_url, '', texto)\n",
        "    return texto_limpio\n",
        "\n",
        "#procesar_tweet(tweet):\n",
        "# Aplicamos todas las funciones anteriores para procesar un tweet\n",
        "# y retornamos el resultado.\n",
        "def procesar_tweet(tweet):\n",
        "    if (tweet == ''):\n",
        "      return ''\n",
        "    contenido = \"\"\n",
        "    contenido = tweet\n",
        "    contenido = contenido.lower()\n",
        "    contenido = strip_accents(contenido)\n",
        "    cambio = True\n",
        "    contenido,cambio = reemplazar_Usuario(contenido)\n",
        "    contenido,cambio = reemplazar_Hashtags(contenido)\n",
        "    contenido,cambio = reemplazar_URL(contenido)\n",
        "    contenido = eliminar_url(contenido)\n",
        "    contenido,cambio = reemplazar_Groserias(contenido)\n",
        "    contenido,cambio = reemplazar_Abreviaturas(contenido)\n",
        "    contenido = reemplazar_repeticiones(contenido)\n",
        "    contenido,cambio = reemplazar_Risa(contenido)\n",
        "    contenido = alfanumericos(contenido)\n",
        "    contenido = remove_stop_words(contenido)\n",
        "    contenido = remover_letras(contenido)\n",
        "    contenido = remover_espacios(contenido)\n",
        "    return contenido\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcWe4NFq96Ap"
      },
      "source": [
        "Respecto al preprocesamiento que realizamos en el laboratorio 1, en esta ocasión eliminaremos las menciones y URLs.\n",
        "Es decir, las sustituimos por el string vacío \"\". En cuanto a Hashtags, solo eliminaremos el símbolo \"#\", y no pondremos la palabra \"HASHTAG\" al reemplazar\n",
        "A su vez, en esta ocasión nos resultará útil volver a pasar los tweets a minúsculas, y optaremos por eliminar las stopwords provistas en el EVA.\n",
        "También eliminaremos los números y tíldes, ya que creemos que estos no aportan al objetivo, y reemplazaremos las distintas malas palabras\n",
        "por la palabra \"insulto\", ya que esta se encuentra en la lista de lemas negativos, por lo que de este modo remarcará de mejor manera oraciones negativas. Además, en vez de preprocesar las risas y cambiarlas por el string 'jaja', decidimos cambiarlo por el string 'jajaja' ya que esta palabra se encuentra incluida en el léxico de palabras positivas y 'jaja' no. Como mencionamos, decidimos volver a eliminar los tildes para uniformizar los tweets ya que no siempre encontraremos las palabras con los tildes correctamente escritos en los tweets, como sucede habitualmente. Ademas, para utilizar los léxicos reconocer las palabras de los mismos la mayor cantidad de veces posible en los tweets. A su vez a diferencia de la tarea 1, no se hace un análisis sintáctico de los tweets, por lo que tildes y mayúsculas no tienen la misma relevancia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZMlbsw2uFLm"
      },
      "source": [
        "# Parte 2 - Representación de los tweets\n",
        "\n",
        "Para representar los tweets se pide que experimenten con modelos basados en Bag of Words (BoW) y con Word Embeddings.\n",
        "\n",
        "Para los dos enfoques podrán elegir entre diferentes opciones:\n",
        "\n",
        "**Bag of Words**\n",
        "\n",
        "* BOW estándar: se recomienda trabajar con la clase [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de sklearn, en particular, fit_transform y transform.\n",
        "* BOW filtrando stop-words: tienen disponible en eva una lista de stop-words para el español, adaptada para análisis de sentimiento (no se filtran palabras relevantes para determinar la polaridad, como \"no\", \"pero\", etc.).\n",
        "* BoW usando lemas: pueden usar herramientas de spacy.\n",
        "* BOW seleccionando las features más relevantes: se recomienda usar la clase [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html?highlight=select%20k%20best#sklearn.feature_selection.SelectKBest) y probar con diferentes valores de k (por ejemplo, 10, 50, 200, 1000).\n",
        "* BOW combinado con TF-IDF: se recomienda usar la clase [TfidfVectorizer](https://https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
        "\n",
        "**Word Embeddings**\n",
        "\n",
        "* A partir de los word embeddings, representar cada tweet como el vector promedio (mean vector) de los vectores de las palabras que lo componen.\n",
        "* A partir de los word embeddings, representar cada tweet como la concatenación de los vectores de las palabras que lo componen (llevando el vector total a un largo fijo).\n",
        "\n",
        "Se recomienda trabajar con alguna de las colecciones de word embeddings disponibles en https://github.com/dccuchile/spanish-word-embeddings. El repositorio incluye links a ejemplos y tutoriales.\n",
        "\n",
        "\n",
        "Se pide que prueben al menos una opción basada en BoW y una basada en word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPSkVX_5wFQE"
      },
      "source": [
        "Imports necesarios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iERG67FGvrG"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h7xiuo1w9rg"
      },
      "source": [
        "## Representación de los Tweets utilizando Bag of Words:\n",
        "  - En primer lugar, decidimos utilizar BOW combiando con TF-IDF, por lo que usamos las clases CountVectorizer y TfidfVectorizer, importadas anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9BNltLduHqB",
        "outputId": "fe979e3c-cac8-48cc-9a91-ab1fd8f2d0d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(18373, 21771)\n",
            "(1884, 21771)\n",
            "(1132, 21771)\n"
          ]
        }
      ],
      "source": [
        "# Representación de los tweets usando BoW\n",
        "\n",
        "#BOW combinado con TF-IDF: utilizamos las clases CountVectorizer y TfidfVectorizer.\n",
        "#preprocesamos los tweets del corpus de entrenamiento\n",
        "def procesar_corpus(corpus):\n",
        "  for j in range(0,len(corpus)):\n",
        "    aux2 = corpus[j]\n",
        "    aux2[1] = procesar_tweet(aux2[1])\n",
        "    corpus[j] = aux2\n",
        "  return corpus\n",
        "\n",
        "\n",
        "train_set2 = train_modificado.copy()                          #Para mantener los data sets intactos\n",
        "devel_set2 = devel_set.copy()\n",
        "test_set_2  = test_set.copy()\n",
        "\n",
        "Train_procesado = procesar_corpus(train_set2)\n",
        "X_trainP = [''.join(words[1]) for words in Train_procesado]  #Tweets de Entrenamiento preprocesados\n",
        "\n",
        "Devel_procesado = procesar_corpus(devel_set2)\n",
        "X_develP = [''.join(words[1]) for words in Devel_procesado]  #Tweets de Desarrollo preprocesados\n",
        "\n",
        "Test_procesado = procesar_corpus(test_set_2)\n",
        "X_testP = [''.join(words[1]) for words in Test_procesado]    #Tweets de Test preprocesados\n",
        "\n",
        "y_train_mod = [label[2] for label in train_set2]             #Etiquetas de Entrenamiento\n",
        "y_devel = [label[2] for label in devel_set2]                  #Etiquetas de Desarrollo\n",
        "y_test = [label[2] for label in test_set_2]                  #Etiquetas de Test\n",
        "\n",
        "#min_df=1 establece que una palabra debe aparecer como minimo en uno de los tweets para ser considerada en el vocabulario.\n",
        "vectorizer = TfidfVectorizer(min_df = 1)\n",
        "\n",
        "X_train_bowP = vectorizer.fit_transform(X_trainP)            #Representamos los datos de entrenamiento en forma de BOW con esquema TF-IDF.\n",
        "X_test_bowP = vectorizer.transform(X_testP)                  #Representamos los datos de test en forma de BOW con esquema TF-IDF.\n",
        "X_devel_bowP = vectorizer.transform(X_develP)                #Representamos los datos de desarrollo en forma de BOW con esquema TF-IDF.\n",
        "\n",
        "print(X_train_bowP.shape)\n",
        "print(X_test_bowP.shape)\n",
        "print(X_devel_bowP.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmEv0aVy1jNb"
      },
      "source": [
        "En las salidas anteriores, vemos como al representar los tweets de los distintos corpus mediante el enfoque de BOW previamente mencionado, queda determinado un vocabulario de 21771 palabras representativas, indicado por el segundo valor de las duplas. Estas 21771 palabras hacen referencia a la unión de las palabras que aparecen en los distintos tweets del corpus de entrenamiento,y los lemas positivos y negativos brindados via EVA. Por otro lado el primer valor representa la cantidad de tweets presentes en cada corpus, donde cada tweet tiene su correspondiente BOW."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGL1OpBQrEyw"
      },
      "source": [
        "Veamos para un tweet random del corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHWgPHttHI-5",
        "outputId": "a2b7fda3-7b89-41ef-8bce-9be938336be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El tweet tiene id: 769980058761068547\n",
            "El tweet es: odisea libros favoritos shakespeare ejemplo encanta hamlet\n",
            "Su categoría: P\n",
            "Luego de procesarlo: odisea libros favoritos shakespeare ejemplo encanta hamlet\n",
            "Representacion en BOW del tweet:\n",
            "  (0, 19160)\t0.42615975578703486\n",
            "  (0, 14942)\t0.42615975578703486\n",
            "  (0, 12737)\t0.34738011670321906\n",
            "  (0, 10415)\t0.42615975578703486\n",
            "  (0, 9104)\t0.35842239580637375\n",
            "  (0, 7683)\t0.30223542075553095\n",
            "  (0, 7396)\t0.3386410766386862\n"
          ]
        }
      ],
      "source": [
        "random_tweet = random.choice(train_set)\n",
        "print(f\"El tweet tiene id: {random_tweet[0]}\")\n",
        "print(f\"El tweet es: {random_tweet[1]}\")\n",
        "print(f\"Su categoría: {random_tweet[2]}\")\n",
        "\n",
        "tweet_procesado = procesar_tweet(random_tweet[1])\n",
        "Aux = [tweet_procesado]\n",
        "print(f\"Luego de procesarlo: {tweet_procesado}\")\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df = 1)\n",
        "X_train_bowPAux = vectorizer.fit_transform(X_trainP)         #con tweets procesados\n",
        "tweet_bow = vectorizer.transform(Aux)\n",
        "print(\"Representacion en BOW del tweet:\")\n",
        "print(tweet_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txBAqaveqtq5"
      },
      "source": [
        "Como se puede ver, para el tweet aleatorio tendremos una cantidad de filas en la representación igual a la cantidad de palabras que cuenta el tweet luego de preprocesarlo. El segundo elemento de la dupla indica el índice de la palabra respecto al inixado que realiza la clase TfidfVectorizer, y a la derecha el correspondiente valor que esta también genera tomando en cuenta el esquema TF-IDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDZMiMalJB3C"
      },
      "source": [
        "- En segundo lugar, optamos por BOWs donde seleccionamos las features mas relevantes, utilizando la clase SelectKBest. En particular se utilizaron los valores k = 250, k = 500 y k = 1000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIHUlTEmJgK3"
      },
      "outputs": [],
      "source": [
        "def seleccionarK(Entrada,Salida,Ka):\n",
        "  vectorizer = CountVectorizer()\n",
        "  bag_of_words = vectorizer.fit_transform(Entrada)\n",
        "  # Seleccionamos Ka características mas relevantes con SelectKBest\n",
        "  k = Ka  # Número de características a seleccionar\n",
        "  selector = SelectKBest(score_func=chi2, k=k)  # Utilizamos chi2 como función de puntuación\n",
        "\n",
        "  selected_features = selector.fit_transform(bag_of_words, Salida)\n",
        "\n",
        "  # Obtenemos el vocabulario y las características seleccionadas\n",
        "  vocab = vectorizer.get_feature_names_out()\n",
        "  selected_feature_indices = selector.get_support(indices=True)\n",
        "  selected_vocab = [vocab[i] for i in selected_feature_indices]\n",
        "  return selected_vocab\n",
        "\n",
        "def representar_con_k(Entrada,Vocabulario_k):\n",
        "  vectorizer = CountVectorizer(vocabulary=Vocabulario_k)\n",
        "  bag_of_words = vectorizer.transform(Entrada)\n",
        "  return bag_of_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKuUMLA8KPBs"
      },
      "source": [
        "Por ejemplo veamos las palabras seleccionadas para el conjunto de Entrenamiento pre-procesado y con léxico negativo y positivo agregados al mismo, con k = 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PmhutH5KgUU",
        "outputId": "6b2e1bd3-6686-4766-be9e-56cbc34e642f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deficit\n",
            "enhorabuena\n",
            "felicidades\n",
            "feliz\n",
            "gracias\n",
            "gran\n",
            "insulto\n",
            "muy\n",
            "no\n",
            "portada\n"
          ]
        }
      ],
      "source": [
        "vocabularioAux = seleccionarK(X_trainP,y_train_mod,10)\n",
        "for t in vocabularioAux:\n",
        "  print(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7x7kb6wKv3B"
      },
      "source": [
        "Ahora declararemos funciones que nos seran de utilidad a a la hora de requerir la representacion en BOW de los distintos Data Sets. Las mismas, son utilizadas de modo que a cada BOW que representa un tweet, se le agregara a dicha representación dos valores mas al final de este, de modo que en la anteúltima coordenada tendremos el numero de palabras en el tweet que se encuentran presentes en el léxico de palabras positivas. Del mismo modo la última coordenada del vector tendrá la cantidad de palabras del tweet que se encuentran en el léxico de palabras negativas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAm8_6Z2LTY-"
      },
      "outputs": [],
      "source": [
        "negativas = []\n",
        "for elem in neg_set:\n",
        "  negativas.insert(0,elem[0])\n",
        "\n",
        "positivas = []\n",
        "for elem in pos_set:\n",
        "  positivas.insert(0,elem[0])\n",
        "\n",
        "\n",
        "def contar_lemas(texto):\n",
        "  tokens = word_tokenize(texto)\n",
        "  pos = [pal[1] for pal in positivas]\n",
        "  neg = [pal2[1] for pal2 in negativas]\n",
        "  suma_pos = 0\n",
        "  suma_neg = 0\n",
        "  for tok in tokens:\n",
        "    if tok in pos:\n",
        "      suma_pos += 1\n",
        "    if tok in neg:\n",
        "      suma_neg +=1\n",
        "  return [suma_pos,suma_neg]\n",
        "\n",
        "\n",
        "def lemas_Conjunto(X):\n",
        "  col = []\n",
        "  for x in X:\n",
        "    dos_valores = contar_lemas(x)\n",
        "    col.append(dos_valores)\n",
        "  return col\n",
        "\n",
        "def agregar_a_bow(BOWS, Extras):\n",
        "  j = 0\n",
        "  for lista in BOWS:    #Largo de ambos es igual\n",
        "    aux = csr_matrix([Extras[j]])\n",
        "    matriz_combinada = hstack([lista, aux])\n",
        "    lista = matriz_combinada\n",
        "    j +=1\n",
        "  return BOWS\n",
        "\n",
        "extras = lemas_Conjunto(X_trainP)\n",
        "extras2 = lemas_Conjunto(X_develP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXCHME7JxdZn"
      },
      "source": [
        "## Representación de los tweets utilizando Word Embeddings:\n",
        "  Decidimos representar los tweets mediante word embeddings siguiendo la sugerencia de tomar el vector promedio y la concatenación de los word embeddings de las palabras de los tweets luego de ser preprocesados.\\\n",
        "  Para cargar el archivo de word embeddings utilizamos Google Drive, cargamos en archivo dentro de una carpeta llamada 'IPLN' en el directorio principal de nuestro drive y dentro colocamos el archivo que se puede obtener desde: https://fasttext.cc/docs/en/crawl-vectors.html en la sección 'Spanish', descargando el archivo .text. Este contiene 2 millones de WE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuDPFFHGq86g"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "\n",
        "\n",
        "#importar_vectores(limite_vectores): carga el archivo de embeddings tomando limite_vectores como limite.\n",
        "def importar_vectores(limite_vectores):\n",
        "  #Cargamos el archivo .vec con los vectores de palabras desde Drive:\n",
        "  drive.mount('/content/drive')\n",
        "  wordvectors_file_vec = '/content/drive/MyDrive/IPLN/cc.es.300.vec'\n",
        "  wordvectors_col = KeyedVectors.load_word2vec_format(wordvectors_file_vec,limit=limite_vectores)\n",
        "  vocabulario_size = wordvectors_col.vectors.shape[0]\n",
        "  print('la colección importada tiene ' + str(vocabulario_size) + ' vectores cargados')\n",
        "  return wordvectors_col\n",
        "\n",
        "\n",
        "\n",
        "#cargar_lexico(lexico): toma un léxico y carga todos sus elementos en una lista\n",
        "def cargar_lexico(lexico):\n",
        "  ret = []\n",
        "  for elem in lexico:\n",
        "    ret.append(procesar_tweet(elem[0]))\n",
        "  return ret\n",
        "lexico_negativas = cargar_lexico(neg_set.copy())\n",
        "lexico_positivas = cargar_lexico(pos_set.copy())\n",
        "\n",
        "#importar_vectores(limite_vectores): carga el archivo de embeddings tomando limite_vectores como limite.\n",
        "def importar_vectores(limite_vectores):\n",
        "  #Cargamos el archivo .vec con los vectores de palabras desde Drive:\n",
        "  drive.mount('/content/drive')\n",
        "  wordvectors_file_vec = '/content/drive/MyDrive/IPLN/cc.es.300.vec'\n",
        "  wordvectors_col = KeyedVectors.load_word2vec_format(wordvectors_file_vec,limit=limite_vectores)\n",
        "  vocabulario_size = wordvectors_col.vectors.shape[0]\n",
        "  print('la colección importada tiene ' + str(vocabulario_size) + ' vectores cargados')\n",
        "  return wordvectors_col\n",
        "\n",
        "\n",
        "\n",
        "#cargar_top_embeddings(limite_vectores,cantidad_top_palabras,corpus):\n",
        "#Obtiene los 'cantidad_top_palabras' embeddings más comunes de 'corpus' y los almacena en un diccionario\n",
        "#de tipo palabra -> embedding, si es que su embedding está definido en la coleccion importada 'col_vectores_importados'.\n",
        "#Si cantidad_top_palabras es 0, devuelve todas las palabras con embedding del corpus\n",
        "#NOTA: El corpus ya debe estar procesado.\n",
        "def cargar_top_embeddings(col_vectores_importados,cantidad_top_palabras,corpus,incluir_lexicos):\n",
        "\n",
        "  #Copiamos el corpus para procesarlo:\n",
        "  corpus_carga = corpus.copy()\n",
        "  X_carga = [''.join(words[1]) for words in corpus_carga]\n",
        "\n",
        "  #Separamos en una lista todas las palabras del corpus:\n",
        "  counter_palabras = Counter([words for tweets in X_carga for words in tweets.split()])\n",
        "  df = pd.DataFrame()\n",
        "  df['key'] = counter_palabras.keys()\n",
        "  df['value'] = counter_palabras.values()\n",
        "  df.sort_values(by='value', ascending=False, inplace=True)\n",
        "\n",
        "  top_n_words = []\n",
        "  #Tomamos las 18.000 palabras mas comunes:\n",
        "  if (cantidad_top_palabras):\n",
        "    top_n_words = list(df[:cantidad_top_palabras].key.values)\n",
        "  else:\n",
        "    top_n_words =  list(df.key.values)\n",
        "\n",
        "  print('Las palabras mas comunes del corpus son: ' + str(top_n_words[:10]))\n",
        "\n",
        "  #Obtenemos la colección de 'cantidad_top_palabras' vectores:\n",
        "  coleccion_vectores_we = {}\n",
        "  for word in top_n_words:\n",
        "    if word in col_vectores_importados.key_to_index:\n",
        "      if(incluir_lexicos):\n",
        "        negativa=0\n",
        "        positiva=0\n",
        "        if word in lexico_negativas:\n",
        "          negativa=1\n",
        "        if word in lexico_positivas:\n",
        "          positiva=1\n",
        "        aux = col_vectores_importados.get_vector(word)\n",
        "        v_lista = aux.tolist()\n",
        "        #Agregamos dos ceros para generar atributos con los léxicos más adelante\n",
        "        v_lista.append(negativa)\n",
        "        v_lista.append(positiva)\n",
        "        coleccion_vectores_we[word] = np.array(v_lista)\n",
        "      else:\n",
        "        aux = col_vectores_importados.get_vector(word)\n",
        "        coleccion_vectores_we[word] = np.array(aux)\n",
        "  print('Nuestra colección tiene ' + str(len(coleccion_vectores_we)) + ' vectores cargados')\n",
        "  return coleccion_vectores_we"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzuMkQ3R120c"
      },
      "source": [
        "Como se puede apreciar con estos ejemplos, se importa la colección de word embeddings y verificamos que se tiene la información semántica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSRNG65LygqT",
        "outputId": "22a259ed-abfd-4775-eb81-9242e4147db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "la colección importada tiene 50000 vectores cargados\n",
            "\n",
            "Ejemplo palabra menos relacionada: [sol,luna,almuerzo,estrellas]: almuerzo\n",
            "\n",
            "Ejemplo palabra menos relacionada: [blanco,azul,rojo,chile]: chile\n",
            "\n",
            "Top 2 mas similares a 'fruta, amarillo' y diferentes a 'color': ['piña', 'plátano']\n",
            "\n",
            "Más similar a 'comida, noche': ['cena']\n"
          ]
        }
      ],
      "source": [
        "#   ####################################################  #\n",
        "#   Ejemplos de representaciones utilizando wordvectors:  #\n",
        "#   ####################################################  #\n",
        "\n",
        "\n",
        "limite_vectores_importados = 50000\n",
        "top_palabras_tweets = 18000\n",
        "wordvectors = importar_vectores(limite_vectores_importados)\n",
        "\n",
        "#Encontrando la palabra menos relacionada:\n",
        "ej1 = wordvectors.doesnt_match(['sol','luna','almuerzo','estrellas'])\n",
        "ej2 = wordvectors.doesnt_match(['blanco','azul','rojo','chile'])\n",
        "print('\\nEjemplo palabra menos relacionada: [sol,luna,almuerzo,estrellas]: ' + str(ej1))\n",
        "print('\\nEjemplo palabra menos relacionada: [blanco,azul,rojo,chile]: ' + str(ej2))\n",
        "\n",
        "#Palabras más similares:\n",
        "\n",
        "ej3 = wordvectors.most_similar_cosmul(positive=['fruta','amarillo'],negative=['color'])\n",
        "\n",
        "print(\"\\nTop 2 mas similares a 'fruta, amarillo' y diferentes a 'color': \" + str([par[0] for par in ej3[:2]]))\n",
        "\n",
        "ej4 = wordvectors.most_similar_cosmul(positive=['comida','noche'])\n",
        "\n",
        "print(\"\\nMás similar a 'comida, noche': \" + str([par[0] for par in ej4[:1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V8ZUK5rfTQj"
      },
      "source": [
        "A modo de ejemplo, imprimimos un vector de embeddings para una palabra del español:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHtsrvaDfiMh",
        "outputId": "01b80b65-6b53-4472-9565-3163c86403c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-5.670e-02  5.340e-02 -6.130e-02 -2.440e-01 -1.366e-01  5.670e-02\n",
            "  5.910e-02 -1.810e-02 -9.960e-02 -1.206e-01 -5.050e-02  7.950e-02\n",
            "  1.053e-01  6.820e-02  1.325e-01  3.310e-02 -4.620e-02  9.190e-02\n",
            " -1.750e-02  7.590e-02  5.230e-02 -5.350e-02  1.290e-02  1.149e-01\n",
            " -4.020e-02  3.160e-02 -1.514e-01  4.190e-02 -6.790e-02  2.320e-02\n",
            "  3.070e-02  8.190e-02 -1.450e-02 -1.122e-01 -5.680e-02  3.400e-02\n",
            " -1.000e-03 -9.070e-02 -1.680e-02  1.009e-01  1.248e-01 -1.486e-01\n",
            " -6.620e-02 -5.230e-02 -2.742e-01  1.379e-01  4.000e-03  9.410e-02\n",
            " -3.590e-02  1.095e-01  6.020e-02  2.642e-01  6.180e-02  3.600e-03\n",
            " -2.710e-02  1.617e-01  6.400e-02 -5.940e-02  2.050e-02 -1.510e-02\n",
            "  9.700e-03  2.760e-02 -1.165e-01  5.110e-02 -4.890e-02 -1.990e-02\n",
            " -2.530e-02  4.650e-02 -3.460e-02  2.451e-01 -1.217e-01  3.800e-02\n",
            "  9.400e-03 -4.520e-02  4.680e-02  3.540e-02  2.700e-02 -1.145e-01\n",
            " -1.400e-03  1.509e-01 -6.750e-02  1.307e-01 -1.050e-01  5.300e-02\n",
            "  1.350e-01 -3.120e-02  1.148e-01  1.210e-02  3.870e-02 -2.524e-01\n",
            "  9.700e-03 -4.570e-02 -1.790e-02  1.415e-01  3.870e-02 -2.370e-02\n",
            "  5.050e-02  4.170e-02 -1.280e-02  4.590e-02 -7.540e-02  6.030e-02\n",
            " -5.710e-02 -1.271e-01  8.250e-02  1.919e-01 -3.540e-02 -9.000e-03\n",
            "  5.460e-02 -1.678e-01 -5.920e-02  2.850e-02 -7.670e-02 -1.620e-02\n",
            " -2.650e-02  1.440e-02  3.110e-02 -8.440e-02  3.830e-01  5.250e-02\n",
            " -1.454e-01 -2.066e-01 -9.100e-02 -2.820e-02 -9.700e-03 -1.146e-01\n",
            " -1.251e-01  5.400e-03  9.740e-02 -7.480e-02 -1.153e-01 -1.477e-01\n",
            "  8.420e-02 -5.560e-02 -2.390e-02  3.520e-02 -1.090e-01  1.610e-02\n",
            "  1.845e-01  7.980e-02  2.330e-02 -1.529e-01  4.690e-02  1.170e-02\n",
            " -9.260e-02  5.830e-02 -1.220e-02 -7.250e-02  1.183e-01 -1.796e-01\n",
            " -8.010e-02 -3.000e-04  4.680e-02 -2.000e-01  6.440e-02  3.410e-02\n",
            "  1.700e-03 -3.770e-02 -4.210e-02  1.000e-04 -1.322e-01  3.580e-02\n",
            " -1.159e-01  2.018e-01  3.230e-02 -2.350e-02 -6.880e-02 -2.050e-02\n",
            " -2.248e-01 -8.540e-02  1.075e-01 -4.730e-02  1.700e-02 -1.930e-02\n",
            " -2.070e-02  1.267e-01  9.670e-02  1.810e-02  1.087e-01 -6.830e-02\n",
            " -1.337e-01  3.990e-02  6.580e-02  1.193e-01 -1.551e-01  2.803e-01\n",
            " -6.680e-02 -5.600e-03 -1.364e-01  8.910e-02 -2.580e-02  1.651e-01\n",
            " -1.254e-01  8.590e-02 -4.690e-02  1.697e-01 -2.273e-01  1.385e-01\n",
            " -7.660e-02  6.510e-02  7.910e-02 -3.180e-02 -1.090e-02  3.040e-02\n",
            " -9.960e-02  1.614e-01  6.040e-02 -3.009e-01  2.990e-02  1.263e-01\n",
            " -1.000e-04  1.193e-01  2.390e-02  9.770e-02  1.694e-01 -5.680e-02\n",
            " -1.337e-01 -2.000e-04  7.200e-02 -9.900e-03  1.131e-01  4.100e-03\n",
            "  6.560e-02  3.470e-02 -7.820e-02 -1.210e-02  5.280e-02  7.160e-02\n",
            "  1.884e-01 -2.060e-02 -2.640e-02  2.430e-02  2.850e-02  1.010e-02\n",
            "  7.000e-02 -8.020e-02 -8.740e-02 -9.100e-03  2.409e-01 -6.650e-02\n",
            "  1.100e-03  7.080e-02  5.320e-02  4.560e-02 -1.700e-01 -6.530e-02\n",
            "  5.760e-02 -4.270e-02  1.995e-01 -4.800e-02  1.604e-01  5.440e-02\n",
            " -4.700e-03  9.160e-02 -8.140e-02 -1.340e-01  1.580e-02  6.020e-02\n",
            "  1.580e-02 -5.850e-02 -1.010e-02 -1.241e-01  3.690e-02  3.260e-02\n",
            "  3.510e-02 -1.880e-02  7.640e-02  1.842e-01  4.760e-02 -8.200e-03\n",
            " -9.580e-02  5.110e-02  8.430e-02  1.850e-02  7.850e-02 -1.535e-01\n",
            " -3.990e-02  1.649e-01  3.600e-02 -4.500e-03  4.880e-02 -1.132e-01\n",
            "  1.626e-01 -1.953e-01  1.070e-02  5.770e-02 -8.110e-02  2.030e-02\n",
            " -5.180e-02 -4.320e-02 -5.160e-02 -4.290e-02  4.370e-02  3.210e-02\n",
            " -8.200e-03  3.980e-02  8.800e-03 -2.150e-02  1.066e-01  3.230e-02]\n"
          ]
        }
      ],
      "source": [
        "print(wordvectors.get_vector('hola'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POcB1tIy0iH8"
      },
      "source": [
        "Importamos la colección de word embeddings y a continuación definimos nuestra propia colección tomando las palabras mas frecuentes de los tweets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cwENSK40miT"
      },
      "outputs": [],
      "source": [
        "limite_vectores_importados = 2000000\n",
        "wordvectors = importar_vectores(limite_vectores_importados)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDdnFcxs7XPi"
      },
      "source": [
        "Creamos las colecciones a utilizar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLZ_dvr51WBF"
      },
      "outputs": [],
      "source": [
        "coleccion_vectores_we =  cargar_top_embeddings(wordvectors,0,Train_procesado,False)\n",
        "Cool = cargar_top_embeddings(wordvectors,0,Train_procesado,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuNfTMASRy9F"
      },
      "outputs": [],
      "source": [
        "#Experimentacion con tweets:\n",
        "#A partir de los word embeddings, representar cada tweet como el vector promedio\n",
        "#(mean vector) de los vectores de las palabras que lo componen.\n",
        "\n",
        "#splitted_tweet(tweet) -> recibe tweet \"crudo\", obtenido directamente de un corpus,\n",
        "#lo preprocesa con la parte 1 y retorna el resultado de dividirlo en palabras con split().\n",
        "def splitted_tweet(tweet):\n",
        "  tweet_procesado = procesar_tweet(tweet)\n",
        "  return tweet_procesado.split()\n",
        "\n",
        "\n",
        "#obtener_vector_promedio(tweet_split) -> Recibe una lista de palabras en \"tweet_split\" y retorna en vector_promedio el vector promedio de la representacion con word\n",
        "#embeddings de cada palabra del tweet y en cant_palabras, la cantidad de palabras del tweet.\n",
        "def obtener_vector_promedio(tweet_split):\n",
        "    vector_promedio = np.zeros(302)\n",
        "    ret = np.zeros(302)\n",
        "    palabras_negativas = 0\n",
        "    palabras_positivas = 0\n",
        "    cant_palabras = 0\n",
        "    for palabra_tweet in tweet_split:\n",
        "      if palabra_tweet in lexico_negativas:\n",
        "          palabras_negativas+=1\n",
        "      if palabra_tweet in lexico_positivas:\n",
        "          palabras_positivas+=1\n",
        "    vectores_tweet = []\n",
        "    vectores_tweet2 = [coleccion_vectores_we[palabra] for palabra in tweet_split if palabra in coleccion_vectores_we]\n",
        "    for v_tweet in vectores_tweet2:\n",
        "      aux = v_tweet.tolist()\n",
        "      aux.append(0)\n",
        "      aux.append(0)\n",
        "      vectores_tweet.append(np.array(aux))\n",
        "    if vectores_tweet:\n",
        "      matriz_palabras = np.array(vectores_tweet)\n",
        "      vector_promedio = np.mean(matriz_palabras, axis=0)\n",
        "      v_lista = np.zeros(302)\n",
        "      v_lista[300] = palabras_negativas\n",
        "      v_lista[301] = palabras_positivas\n",
        "      ret = vector_promedio + np.array(v_lista)\n",
        "    return ret,cant_palabras\n",
        "\n",
        "#promedio_de_palabras(corpus) -> obtiene el promedio de las palabras del corpus corpus\n",
        "#luego de este ser procesado\n",
        "# NOTA: pasar un corpus copiado como parámetro.\n",
        "def promedio_de_palabras(corpus):\n",
        "  corpus_procesado = procesar_corpus(corpus)\n",
        "  corpus_tweets = [x[1] for x in corpus_procesado]\n",
        "  for tweet in corpus_tweets:\n",
        "    t_split = tweet.split()\n",
        "    for palabra in t_split:\n",
        "      if(not (palabra in coleccion_vectores_we)):\n",
        "        t_split.remove(palabra)\n",
        "    tweet = (''.join(elem) for elem in t_split)\n",
        "  cantidad_tweets = len(corpus_tweets)\n",
        "  counter = [words for tweets in X_trainP for words in tweets.split()]\n",
        "  return len(counter) / (cantidad_tweets)\n",
        "\n",
        "#Definimos el largo máximo de la representación de las palabras\n",
        "\n",
        "largo_maximo_en_palabras = math.trunc(promedio_de_palabras(train_set.copy()))\n",
        "print('Largo promedio de las palabras de train set entrenado: ' + str(largo_maximo_en_palabras))\n",
        "\n",
        "#obtener_vector_concatenacion(tweet_split): tweet split es una lista de strings de palabras, obtiene\n",
        "#la representacion en concatenacion de los word embeddings de esa lista.\n",
        "def obtener_vector_concatenacion(tweet_split):\n",
        "  vector_concatenado = np.zeros(largo_maximo_en_palabras*300+2,dtype=np.float64)\n",
        "  palabras_negativas = 0\n",
        "  palabras_positivas = 0\n",
        "  cant_palabras = 0\n",
        "  vectores_tweet = []\n",
        "  vectores_tweet2 = [coleccion_vectores_we[palabra] for palabra in tweet_split if palabra in coleccion_vectores_we]\n",
        "  for v_tweet in vectores_tweet2:\n",
        "    aux = v_tweet.tolist()\n",
        "    aux.append(0)\n",
        "    aux.append(0)\n",
        "    vectores_tweet.append(np.array(aux))\n",
        "  if(vectores_tweet):\n",
        "    vector_count = np.array(vectores_tweet[0])\n",
        "    vectores_tweet.pop(0)\n",
        "    for vector_tweet in vectores_tweet:\n",
        "      vector_count = np.concatenate((vector_count, np.array(vector_tweet)))\n",
        "    vector_concatenado = vector_count\n",
        "    diferencia = largo_maximo_en_palabras*300+2-vector_concatenado.shape[0]\n",
        "    if diferencia > 0:\n",
        "      vector_concatenado = np.pad(vector_concatenado, (0, largo_maximo_en_palabras*300+2-vector_concatenado.shape[0]), 'constant')\n",
        "    else:\n",
        "      vector_concatenado = vector_concatenado[:(largo_maximo_en_palabras*300+2)]\n",
        "  for palabra_tweet in tweet_split:\n",
        "      cant_palabras +=1\n",
        "      if palabra_tweet in lexico_negativas:\n",
        "          palabras_negativas+=1\n",
        "      if palabra_tweet in lexico_positivas:\n",
        "          palabras_positivas+=1\n",
        "  vector_concatenado[largo_maximo_en_palabras*300] = palabras_negativas\n",
        "  vector_concatenado[largo_maximo_en_palabras*300+1] = palabras_positivas\n",
        "  return vector_concatenado\n",
        "\n",
        "\n",
        "\n",
        "#word_embedding_promedio_corpus(corpus) Toma \"corpus\" el cual es una lista cargada desde la parte 1\n",
        "#de la forma [[id,tweet,etiqueta],[id2,tweet2,etiqueta2], ... ]\n",
        "#y devuelve el mismo corpus\n",
        "#[[id,tweet_we,etiqueta],[id2,tweet2_we,etiqueta2], ... ] pero los elementos tweet_we son las representaciones\n",
        "#en word embedding tomando el promedio de las palabras reconocidas del tweet\n",
        "def word_embedding_promedio_corpus(corpus):\n",
        "  corpus_emb = corpus.copy()\n",
        "  we_corpus = []\n",
        "  if(len(corpus_emb[0]) == 3):\n",
        "    for elemento in corpus_emb:\n",
        "      tweet = elemento[1]\n",
        "      id = elemento[0]\n",
        "      polaridad = elemento[2]\n",
        "      if(not isinstance(tweet, list)):\n",
        "        tweet_split = splitted_tweet(tweet)\n",
        "        v_prom,n_palabras = obtener_vector_promedio(tweet_split)\n",
        "        we_corpus.append([id,v_prom,polaridad])\n",
        "      else:\n",
        "        print(tweet)\n",
        "  else:\n",
        "    for elemento in corpus_emb:\n",
        "      tweet = elemento[0]\n",
        "      if(not isinstance(tweet, list)):\n",
        "        tweet_split = splitted_tweet(tweet)\n",
        "        v_prom,n_palabras = obtener_vector_promedio(tweet_split)\n",
        "        we_corpus.append([v_prom])\n",
        "      else:\n",
        "        print(tweet)\n",
        "  return we_corpus\n",
        "\n",
        "\n",
        "#word_embedding_concatenacion_corpus(corpus) Toma \"corpus\" el cual es una lista cargada desde la parte 1\n",
        "#de la forma [[id,tweet,etiqueta],[id2,tweet2,etiqueta2], ... ]\n",
        "#y devuelve el mismo corpus\n",
        "#[[id,tweet_we,etiqueta],[id2,tweet2_we,etiqueta2], ... ] pero los elementos tweet_we son las representaciones\n",
        "#en word embedding tomando la concatenacion de las palabras reconocidas del tweet\n",
        "def word_embedding_concatenacion_corpus(corpus):\n",
        "  corpus_emb = corpus.copy()\n",
        "  we_corpus = []\n",
        "  for elemento in corpus_emb:\n",
        "    tweet = elemento[1]\n",
        "    id = elemento[0]\n",
        "    polaridad = elemento[2]\n",
        "    if(not isinstance(tweet, list)):\n",
        "      tweet_split = splitted_tweet(tweet)\n",
        "      v_prom = obtener_vector_concatenacion(tweet_split)\n",
        "      we_corpus.append([id,v_prom,polaridad])\n",
        "    else:\n",
        "      print(tweet)\n",
        "  return we_corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yXYj__F45Mu"
      },
      "source": [
        "Procesamos los corpus y cargamos los datos a utilizar en las siguientes secciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXb5Nz8mbnFV"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "################ Carga de datos para WE: #############\n",
        "######################################################\n",
        "\n",
        "\n",
        "##################### PROMEDIO ########################################\n",
        "\n",
        "#Cargamos los corpus:\n",
        "devel_set_we_promedio = []\n",
        "train_set_we_promedio = []\n",
        "test_set_we_promedio = []\n",
        "train_set_lexicos_we_promedio = []\n",
        "\n",
        "devel_set_we_promedio  = word_embedding_promedio_corpus(devel_set)\n",
        "train_set_we_promedio  = word_embedding_promedio_corpus(train_set)\n",
        "train_set_lexicos_we_promedio  = word_embedding_promedio_corpus(train_set_lexicos)\n",
        "test_set_we_promedio   = word_embedding_promedio_corpus(test_set)\n",
        "\n",
        "\n",
        "#Obtenemos por separado representaciones de los tweets:\n",
        "\n",
        "X_train_lexicos_we_promedio = np.array([x[1] for x in train_set_lexicos_we_promedio])       # tweets train + léxicos\n",
        "X_devel_we_promedio = np.array([x[1] for x in devel_set_we_promedio])                       # tweets devel\n",
        "X_test_we_promedio = np.array([x[1] for x in test_set_we_promedio])                         # tweets test\n",
        "X_train_we_promedio = np.array([x[1] for x in train_set_we_promedio])                       # tweets train\n",
        "\n",
        "\n",
        "################################### CONCATENACION ##########################################\n",
        "\n",
        "#Cargamos los corpus:\n",
        "devel_set_we_concatenacion = []\n",
        "train_set_we_concatenacion = []\n",
        "test_set_we_concatenacion = []\n",
        "train_set_lexicos_we_concatenacion = []\n",
        "\n",
        "devel_set_we_concatenacion   = word_embedding_concatenacion_corpus(devel_set)\n",
        "train_set_we_concatenacion  = word_embedding_concatenacion_corpus(train_set)\n",
        "test_set_we_concatenacion   = word_embedding_concatenacion_corpus(test_set)\n",
        "train_set_lexicos_we_concatenacion  = word_embedding_concatenacion_corpus(train_set_lexicos)\n",
        "\n",
        "X_train_lexicos_we_concatenacion = np.array([x[1] for x in train_set_lexicos_we_concatenacion])     # tweets train + léxicos\n",
        "X_devel_we_concatenacion = np.array([x[1] for x in devel_set_we_concatenacion])                     # tweets devel\n",
        "X_test_we_concatenacion = np.array([x[1] for x in test_set_we_concatenacion])                       # tweets test\n",
        "X_train_we_concatenacion = np.array([x[1] for x in train_set_we_concatenacion])                     # tweets train\n",
        "\n",
        "#################################### ETIQUETAS ##########################################\n",
        "\n",
        "y_train_lexicos_we_promedio = [x[2] for x in train_set_lexicos_we_promedio]                 # etiquetas train + léxicos\n",
        "y_devel_we_promedio = [x[2] for x in devel_set_we_promedio]                                 # etiquetas devel\n",
        "y_test_we_promedio = [x[2] for x in test_set_we_promedio]                                   # etiquetas test\n",
        "y_train_we_promedio = [x[2] for x in train_set_we_promedio]                                 # etiquetas train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxAcPhqgWHr2"
      },
      "source": [
        "# Parte 3 - Clasificación de los tweets\n",
        "\n",
        "Para la clasificación de los tweets es posible trabajar con dos enfoques diferentes:\n",
        "\n",
        "* Aprendizaje Automático basado en atributos: se pide probar al menos dos modelos diferentes, por ejemplo, Multi Layer Perceptron ([MLP](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)) y Support Vector Machines ([SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)), y usar al menos dos formas de representación de tweets (una basada en BoW y otra basada en word embeddings). Se publicó en eva un léxico de palabras positivas y negativas que puede ser utilizado para generar atributos.\n",
        "\n",
        "* Aprendizaje Profundo: se recomienda experimentar con alguna red recurrente como LSTM. En este caso deben representar los tweets an base a word embeddings.\n",
        "\n",
        "Deberán usar el corpus de desarrollo (devel.csv) para comparar resultados de diferentes experimentos, variando los valores de los hiperparámetros, la forma de representación de los tweets, el preprocesamiento, los modelos de AA, etc.\n",
        "\n",
        "Tanto para la evaluación sobre desarrollo como para la evaluación final sobre test se usará la medida [Macro-F1](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) (promedio de la medida F1 de cada clase)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t13agznj4UZE"
      },
      "source": [
        "Imports necesarios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBWa2ayY4Uuo"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler                      #No deberia ser util -> tampoco\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV   #No deberia ser util -> tampoco\n",
        "import pandas as pd                                     #Revisar utilidad -> Tampoco\n",
        "import matplotlib.pyplot as plt                         #Lo mismo -> No lo uso\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import load_iris                  #Revisar -> se usaba en versiones viejas de LSTM\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Reshape\n",
        "from keras.optimizers import RMSprop\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLPStRiv9W1G"
      },
      "source": [
        "Declaramos funciones de utilidad para evaluar los clasificadores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FrqPJn99aVc"
      },
      "outputs": [],
      "source": [
        "#codificar_etiquetas(y): y es una lista con etiquetas, de la forma ['P','N','NONE','P, ...] donde cada etiqueta es un string que\n",
        "#puede ser 'P', 'N' o 'NONE', retorna un np.array que codifica las etiquetas a numeros segun el mapeo:\n",
        "#'P': 0, 'N': 1, 'NONE': 2\n",
        "def codificar_etiquetas(y):\n",
        "  etiquetas = {'P': 0, 'N': 1, 'NONE': 2}\n",
        "  return np.array([etiquetas[i] for i in y])\n",
        "\n",
        "#imprimir_resultados(f1_scores) -> f1_scores es una tripla de índices, imprime los porcentajes Positivos, Negativos, Neutros\n",
        "def imprimir_resultados(puntajes):\n",
        "  print(\"F1 (P):    \" +str(puntajes[0]))\n",
        "  print(\"F1 (N):    \" +str( puntajes[1]))\n",
        "  print(\"F1 (NONE): \" + str(puntajes[2]))\n",
        "  print(\"Macro-F1: \" + str((puntajes[2] + puntajes[1] + puntajes[0])/3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "539GsGVx6GAe"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEBusePnuf0V"
      },
      "source": [
        "### Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M-3hh_EM7sj"
      },
      "source": [
        "#### BOW combinado con TF-IDF:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7F2fMQmNaCm"
      },
      "source": [
        "Primero realizamos pruebas para determinar los mejores valores de ciertos hiperparámetros a ser utilizados para obtener las mejores métricas posibles con este método de aprendizaje automático basado en atributos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "uBfaeKNNhJGN",
        "outputId": "63d970af-36ed-41d3-88c3-4b479f52c66f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El tiempo requerido para completar la GridSearch es de 3452.12 segundos.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (3,)}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultado de Cross-Validation (k=5) del mejor estimador: 0.407\n"
          ]
        }
      ],
      "source": [
        "model_mlp = MLPClassifier(random_state=1234)\n",
        "param_dict = {'hidden_layer_sizes': [(1,), (3,), (10,)],\n",
        "              'activation': ['relu', 'logistic'],\n",
        "              'alpha': [0.001, 0.01]}\n",
        "import warnings\n",
        "with warnings.catch_warnings():             # Ignoramos advertencias dentro del bloque with\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    start = time()\n",
        "    grid_search = GridSearchCV(model_mlp, param_dict, scoring='f1_macro')\n",
        "    grid_search.fit(X_train_bowP, y_train_mod)\n",
        "    print(\"El tiempo requerido para completar la GridSearch es de %.2f segundos.\" % (time()-start))\n",
        "    display(grid_search.best_params_)\n",
        "    print(\"Resultado de Cross-Validation (k=5) del mejor estimador: %.3f\" % grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlrW7NCWOBfH"
      },
      "source": [
        "A continuación ejecutamos con distintos valores, donde podemos ver que efectivamente se cumple que la combinación hidden_layer_sizes = 3, activation = \"logistic\" y alpha = 0.01 es de la que logra mejores resultados, al igual que cuando usamos los mismos datos pero con valor 20 para hidden_layer_sizes. No agregamos pruebas con activation=\"relu\" ya que el rendimiento era bajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzVyMCXNt_bx"
      },
      "outputs": [],
      "source": [
        "def mlp (Ocultas, Alfa, Activacion):\n",
        "  if Alfa == 0:    # No se usan los hiperparametros \"alpha\" ni \"activation\"\n",
        "    clasificador_mlpAux = MLPClassifier(hidden_layer_sizes=(Ocultas,), max_iter=10000,random_state=30)\n",
        "  else:\n",
        "    clasificador_mlpAux = MLPClassifier(hidden_layer_sizes=(Ocultas,), max_iter=10000,random_state=30, alpha=Alfa,activation=Activacion)\n",
        "  clasificador_mlpAux.fit(X_train_bowP, y_train_mod)\n",
        "  y_pred_devel_BOW_MLP = clasificador_mlpAux.predict(X_devel_bowP)           #BOWs de Corpus Test pre-procesado\n",
        "  average_def = None\n",
        "  f1_devel_BOW_MLP = f1_score(y_devel, y_pred_devel_BOW_MLP,average=\"macro\")\n",
        "  return f1_devel_BOW_MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvRyNkVTwUG9",
        "outputId": "5893cd7a-817e-4bbf-cda4-c46132254e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro-F1 para hidden_layer_sizes=3:  0.5128484176918356\n",
            "Macro-F1 para hidden_layer_sizes=10:  0.5359836285230957\n",
            "Macro-F1 para hidden_layer_sizes=20:  0.5134017494869968\n",
            "Macro-F1 para hidden_layer_sizes=3, alpha=0.01 y activation=logistic:  0.5539079436598462\n",
            "Macro-F1 para hidden_layer_sizes=3, alpha=0.001 y activation=logistic:  0.5159206945279009\n",
            "Macro-F1 para hidden_layer_sizes=10, alpha=0.01 y activation=logistic:  0.5531464568317209\n",
            "Macro-F1 para hidden_layer_sizes=10, alpha=0.001 y activation=logistic:  0.5220898448075021\n",
            "Macro-F1 para hidden_layer_sizes=20, alpha=0.01 y activation=logistic:  0.5557007990001974\n",
            "Macro-F1 para hidden_layer_sizes=20, alpha=0.001 y activation=logistic:  0.5168146446810935\n"
          ]
        }
      ],
      "source": [
        "print(\"Macro-F1 para hidden_layer_sizes=3: \", mlp(3,0,\"\"))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=10: \", mlp(10,0,\"\"))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=20: \", mlp(20,0,\"\"))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=3, alpha=0.01 y activation=logistic: \", mlp(3,0.01,\"logistic\"))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=3, alpha=0.001 y activation=logistic: \", mlp(3,0.001,\"logistic\"))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=10, alpha=0.01 y activation=logistic: \", mlp(10,0.01,\"logistic\"))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=10, alpha=0.001 y activation=logistic: \", mlp(10,0.001,\"logistic\"))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=20, alpha=0.01 y activation=logistic: \", mlp(20,0.01,\"logistic\"))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=20, alpha=0.001 y activation=logistic: \", mlp(20,0.001,\"logistic\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tEieu1-vtj0"
      },
      "source": [
        "Como se puede observar, es levemente mejor, por lo que decidimos a partir de este momento no solo consideraremos dichos valores obtenidos en la prueba basada en Cross-Validation para MLP, sino que recurriremos a ensayo y error en busca de los mejores resultados posibles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czM9aJo1QNOW"
      },
      "source": [
        "#### BOW para k-features mas relevantes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy6w14EpQWxE"
      },
      "source": [
        "A cada BOW que representa un tweet, se le agrega a dicha representación dos valores mas al final de este, de modo que en la anteúltima coordenada tendremos el numero de palabras en el tweet que se encuentran presentes en el léxico de palabras positivas. Del mismo modo la ultima coordenada del vector tendra la cantidad de palabras del tweet que se encuentran en el léxico de palabras negativas.\n",
        "\n",
        "Definimos una funcion a continuación para estudiar como se comporta MLP ante tal representación de los tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzjUblqNFFyH"
      },
      "outputs": [],
      "source": [
        "def mlp_bow_k (Ocultas, Alfa, Activacion,ValorK):\n",
        "  if Alfa == 0:    # No se usan los hiperparametros \"alpha\" ni \"activation\"\n",
        "    clasificador_mlp_bowk1 = MLPClassifier(hidden_layer_sizes=(Ocultas,), max_iter=10000,random_state=30)\n",
        "  else:\n",
        "    clasificador_mlp_bowk1 = MLPClassifier(hidden_layer_sizes=(Ocultas,), max_iter=10000,random_state=30, alpha=Alfa,activation=Activacion)\n",
        "  vocabulario = seleccionarK(X_trainP,y_train_mod,ValorK)\n",
        "\n",
        "  X_train_bow_k_mlp = representar_con_k(X_trainP,vocabulario)\n",
        "  #extras = lemas_Conjunto(X_trainP)\n",
        "  X_train_bow_k_mlp_extendido = agregar_a_bow(X_train_bow_k_mlp,extras)\n",
        "\n",
        "  clasificador_mlp_bowk1.fit(X_train_bow_k_mlp_extendido, y_train_mod)\n",
        "\n",
        "  X_devel_bow_k_mlp = representar_con_k(X_develP,vocabulario)\n",
        "  #extras2 = lemas_Conjunto(X_develP)\n",
        "  X_devel_bow_k_mlp_extendido = agregar_a_bow(X_devel_bow_k_mlp,extras2)\n",
        "\n",
        "  y_pred_test_BOW_K_MLP = clasificador_mlp_bowk1.predict(X_devel_bow_k_mlp_extendido)           #BOWs de Corpus Test pre-procesado\n",
        "  average_def = None\n",
        "\n",
        "  f1_devel_BOW_K_MLP = f1_score(y_devel, y_pred_test_BOW_K_MLP,average=\"macro\")\n",
        "  return f1_devel_BOW_K_MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR3HMbenF7gC",
        "outputId": "c6d901e3-c425-47a9-d880-068591a31f9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro-F1 para hidden_layer_sizes=3, alpha=0.01, activation=logistib y k=250:  0.5472966598294682\n",
            "Macro-F1 para hidden_layer_sizes=3, alpha=0.01, activation=logistic y k=500:  0.5615829079500244\n",
            "Macro-F1 para hidden_layer_sizes=3, alpha=0.01, activation=logistic y k=1000:  0.5542923619111257\n",
            "Macro-F1 para hidden_layer_sizes=8, alpha=0.01, activation=logistic y k=250:  0.5399119892961677\n",
            "Macro-F1 para hidden_layer_sizes=8, alpha=0.01, activation=logistic y k=500:  0.5580513107947079\n",
            "Macro-F1 para hidden_layer_sizes=8, alpha=0.01, activation=logistic y k=1000:  0.5530611064704959\n",
            "Macro-F1 para hidden_layer_sizes=20, alpha=0.01, activation=logistic y k=250:  0.5449202561118403\n",
            "Macro-F1 para hidden_layer_sizes=20, alpha=0.01, activation=logistic y k=500:  0.556950140704333\n",
            "Macro-F1 para hidden_layer_sizes=20, alpha=0.01, activation=logistic y k=1000:  0.5073824397096428\n"
          ]
        }
      ],
      "source": [
        "print(\"Macro-F1 para hidden_layer_sizes=3, alpha=0.01, activation=logistib y k=250: \", mlp_bow_k(3,0.01,\"logistic\",250))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=3, alpha=0.01, activation=logistic y k=500: \", mlp_bow_k(3,0.01,\"logistic\",500))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=3, alpha=0.01, activation=logistic y k=1000: \", mlp_bow_k(3,0.01,\"logistic\",1000))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=8, alpha=0.01, activation=logistic y k=250: \", mlp_bow_k(8,0.01,\"logistic\",250))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=8, alpha=0.01, activation=logistic y k=500: \", mlp_bow_k(8,0.01,\"logistic\",500))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=8, alpha=0.01, activation=logistic y k=1000: \", mlp_bow_k(8,0.01,\"logistic\",1000))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=20, alpha=0.01, activation=logistic y k=250: \", mlp_bow_k(20,0.01,\"logistic\",250))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=20, alpha=0.01, activation=logistic y k=500: \", mlp_bow_k(20,0.01,\"logistic\",500))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=20, alpha=0.01, activation=logistic y k=1000: \", mlp_bow_k(20,0.01,\"logistic\",1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwR0Zb5cXE_Y"
      },
      "source": [
        "Nuevamente los mejores valores de Macro-F1 se atribuyen a hidden_layer_sizes = 3, alpha = 0.01 y activation = \"logistic\". A su vez se destaca que el mejor rendimiento del modelo es cuando agregamos las dos coordendas a los BOWs con las 500 features mas relevantes del Data Set de entrenamiento, con Macro-F1 = 0.562. A su vez, con este enfoque se reduce significativamente el tamaño de los vectores: de dimensión mayor a 21.000 con el enfoque TF-IDF a 500 para el caso anterior con enfoque Select k-best, obteniendo mejores resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbUtpyhYJqv_"
      },
      "source": [
        "Veamos ahora que sucede si no agregamos las dos coordenadas extras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFVBgS4OJx5_"
      },
      "outputs": [],
      "source": [
        "def mlp_bow_k_sin_extras (Ocultas, Alfa, Activacion,ValorK):\n",
        "  if Alfa == 0:    # No se usan los hiperparametros \"alpha\" ni \"activation\"\n",
        "    clasificador_mlp_bowk2 = MLPClassifier(hidden_layer_sizes=(Ocultas,), max_iter=10000,random_state=30)\n",
        "  else:\n",
        "    clasificador_mlp_bowk2 = MLPClassifier(hidden_layer_sizes=(Ocultas,), max_iter=10000,random_state=30, alpha=Alfa,activation=Activacion)\n",
        "  vocabulario = seleccionarK(X_trainP,y_train_mod,ValorK)\n",
        "  X_train_bow_k_mlp2 = representar_con_k(X_trainP,vocabulario)\n",
        "\n",
        "  clasificador_mlp_bowk2.fit(X_train_bow_k_mlp2, y_train_mod)\n",
        "\n",
        "  X_devel_bow_k_mlp2 = representar_con_k(X_develP,vocabulario)\n",
        "\n",
        "  y_pred_test_BOW_K_MLP2 = clasificador_mlp_bowk2.predict(X_devel_bow_k_mlp2)           #BOWs de Corpus Test pre-procesado\n",
        "  average_def = None\n",
        "\n",
        "  f1_devel_BOW_K_MLP2 = f1_score(y_devel, y_pred_test_BOW_K_MLP2,average=\"macro\")\n",
        "  return f1_devel_BOW_K_MLP2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1Eo7o74Kgik",
        "outputId": "d63c6c15-85a6-41f6-848f-d394cf7d2eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro-F1 para hidden_layer_sizes=3, alpha=0.01, activation=logistib y k=250:  0.5472966598294682\n",
            "Macro-F1 para hidden_layer_sizes=3, alpha=0.01, activation=logistic y k=500:  0.5615829079500244\n",
            "Macro-F1 para hidden_layer_sizes=3, alpha=0.01, activation=logistic y k=1000:  0.5542923619111257\n",
            "Macro-F1 para hidden_layer_sizes=8, alpha=0.01, activation=logistic y k=250:  0.5399119892961677\n",
            "Macro-F1 para hidden_layer_sizes=8, alpha=0.01, activation=logistic y k=500:  0.5580513107947079\n",
            "Macro-F1 para hidden_layer_sizes=8, alpha=0.01, activation=logistic y k=1000:  0.5530611064704959\n",
            "Macro-F1 para hidden_layer_sizes=20, alpha=0.01, activation=logistic y k=250:  0.5449202561118403\n",
            "Macro-F1 para hidden_layer_sizes=20, alpha=0.01, activation=logistic y k=500:  0.556950140704333\n",
            "Macro-F1 para hidden_layer_sizes=20, alpha=0.01, activation=logistic y k=1000:  0.5073824397096428\n"
          ]
        }
      ],
      "source": [
        "print(\"Macro-F1 para hidden_layer_sizes=3, alpha=0.01, activation=logistic y k=250: \", mlp_bow_k_sin_extras(3,0.01,\"logistic\",250))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=3, alpha=0.01, activation=logistic y k=500: \", mlp_bow_k_sin_extras(3,0.01,\"logistic\",500))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=3, alpha=0.01, activation=logistic y k=1000: \", mlp_bow_k_sin_extras(3,0.01,\"logistic\",1000))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=8, alpha=0.01, activation=logistic y k=250: \", mlp_bow_k_sin_extras(8,0.01,\"logistic\",250))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=8, alpha=0.01, activation=logistic y k=500: \", mlp_bow_k_sin_extras(8,0.01,\"logistic\",500))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=8, alpha=0.01, activation=logistic y k=1000: \", mlp_bow_k_sin_extras(8,0.01,\"logistic\",1000))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=20, alpha=0.01, activation=logistic y k=250: \", mlp_bow_k_sin_extras(20,0.01,\"logistic\",250))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=20, alpha=0.01, activation=logistic y k=500: \", mlp_bow_k_sin_extras(20,0.01,\"logistic\",500))\n",
        "print(\"Macro-F1 para hidden_layer_sizes=20, alpha=0.01, activation=logistic y k=1000: \", mlp_bow_k_sin_extras(20,0.01,\"logistic\",1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydZnfZRFK0N5"
      },
      "source": [
        "Viendo la salida anterior, resulta que si agregamos las dos coordendas extras obtenemos los mismos resultados que al no hacerlo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac86s966XPRx"
      },
      "source": [
        "Vemos que usando 8 capas ocultas, alpha = 0.01 y función de activación logistic, obtenemos un mejor rendimiento que utilizando BOWs combinados con Tf-Idf, para vectores de menor tamaño. Por lo tanto, sin agregar las coordendas es conveniente utilizar el enfoque de selección de las 500 características mas relevantes del Corpus Train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFfpoFyQuOEH"
      },
      "source": [
        "### Word Embeddings:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blZOHl85gIuT"
      },
      "source": [
        "Codificamos las etiquetas de los corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YhbiNAfgHlY"
      },
      "outputs": [],
      "source": [
        "y_train_lexicos_numerico_we = codificar_etiquetas(y_train_lexicos_we_promedio)\n",
        "y_devel_numerico_we         = codificar_etiquetas(y_devel_we_promedio)\n",
        "y_test_numerico_we          = codificar_etiquetas(y_test_we_promedio)\n",
        "y_train_numerico_we         = codificar_etiquetas(y_train_we_promedio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb1Dp929VBsl"
      },
      "source": [
        "#### Word embeddings tomando el promedio:\n",
        "Para esta parte vamos a utilizar word embeddings de tamaño de vector 302, en donde los primeros 300 valores de cada vector de la entrada son el promedio de los vectores obtenidos de la colección importada y los últimos 2 valores son la cantidad de palabras positivas y negativas identificadas en cada tweet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gec8iDkq8_lI"
      },
      "source": [
        "Fase de testeo para obtener los mejores parámetros para la función:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAI-7G9i8LTh"
      },
      "outputs": [],
      "source": [
        "mlp_we_promedio = MLPClassifier(random_state=20)\n",
        "param_dict = {'hidden_layer_sizes': [(1,), (3,), (10,),(100,),(200,)],\n",
        "              'activation': ['relu', 'logistic'],\n",
        "              'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "              'max_iter':[1000]}\n",
        "start = time()\n",
        "grid_search = GridSearchCV(mlp_we_promedio, param_dict, scoring='f1_macro')\n",
        "grid_search.fit(X_train_lexicos_we_promedio, y_train_lexicos_numerico_we)\n",
        "print(\"El tiempo requerido para completar la GridSearch fué de %.2f segundos.\" % (time()-start))\n",
        "display(grid_search.best_params_)\n",
        "print(\"Resultado de Cross-Validation (k=5) del mejor estimador: %.3f\" % grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGJSZI0D96wQ"
      },
      "source": [
        "Creamos el clasificador con los mejores parámetros obtenidos de la parte anterior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "OwEWlMRcXvd-",
        "outputId": "d90c981a-c170-477d-a076-267285dd5b5b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.01, hidden_layer_sizes=(200,), max_iter=1000,\n",
              "              random_state=20)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.01, hidden_layer_sizes=(200,), max_iter=1000,\n",
              "              random_state=20)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(alpha=0.01, hidden_layer_sizes=(200,), max_iter=1000,\n",
              "              random_state=20)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlp_we_promedio = MLPClassifier(hidden_layer_sizes=(200,), max_iter=1000,alpha = 0.01, random_state=20,activation='relu')\n",
        "mlp_we_promedio.fit(X_train_lexicos_we_promedio, y_train_lexicos_numerico_we)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xruGrpOh8LTh",
        "outputId": "5370ddce-a4b0-4974-b3df-b25ac77815c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultados en el corpus devel: \n",
            "F1 (P):    0.6469194312796208\n",
            "F1 (N):    0.5874125874125874\n",
            "F1 (NONE): 0.5078014184397163\n",
            "Macro-F1: 0.5807111457106414\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################################################\n",
        "#####################  Cálculo de medida F1  #####################\n",
        "##################################################################\n",
        "\n",
        "\n",
        "#Hacemos predicciones en el conjunto de prueba:\n",
        "y_pred_devel_mlp_we = mlp_we_promedio.predict(X_devel_we_promedio)\n",
        "\n",
        "#Calculamos los Puntajes f1\n",
        "MLP_f1_devel = f1_score(y_devel_numerico_we, y_pred_devel_we,average=None)\n",
        "\n",
        "#Mostramos los resultados en consola\n",
        "print(\"Resultados en el corpus devel: \")\n",
        "imprimir_resultados(MLP_f1_devel)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o49MgHouVJLV"
      },
      "source": [
        "#### Word Embeddings tomando la concatenación:\n",
        "Ahora, vamos a utilizar word embeddings de tamaño de vector $300 \\times M+2$,\n",
        "siendo M la cantidad máxima de palabras en cada tweet, esta se encuentra definida como 10 mas arriba, en la función de la parte 2. Tomamos este valor ya que es el promedio de las palabras de los tweets del corpus train_set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "fdqN8EINXsIe",
        "outputId": "a00f753f-e86e-40f8-bcad-e619d0410d2c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.01, hidden_layer_sizes=(200,), max_iter=1000,\n",
              "              random_state=20)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.01, hidden_layer_sizes=(200,), max_iter=1000,\n",
              "              random_state=20)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(alpha=0.01, hidden_layer_sizes=(200,), max_iter=1000,\n",
              "              random_state=20)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlp_we_concatenacion = MLPClassifier(hidden_layer_sizes=(200,), max_iter=1000,alpha = 0.01, random_state=20,activation='relu')\n",
        "mlp_we_concatenacion.fit(X_train_lexicos_we_concatenacion, y_train_lexicos_numerico_we)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwBaNo80Xw5-",
        "outputId": "14148b9e-3517-49b0-d20b-e2d734ef8397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultados en el corpus devel: \n",
            "F1 (P):    0.6004962779156328\n",
            "F1 (N):    0.5770862800565771\n",
            "F1 (NONE): 0.5059920106524634\n",
            "Macro-F1: 0.561191522874891\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################################################\n",
        "#####################  Cálculo de medida F1  #####################\n",
        "##################################################################\n",
        "\n",
        "#Hacemos predicciones en el conjunto de prueba:\n",
        "y_pred_devel_we_mlp_concat = mlp_we_concatenacion.predict(X_devel_we_concatenacion)\n",
        "y_pred_test_we_mlp_concat = mlp_we_concatenacion.predict(X_test_we_concatenacion)\n",
        "\n",
        "average_def = None\n",
        "\n",
        "\n",
        "#Calculamos los Puntajes f1\n",
        "MLP_f1_devel = f1_score(y_devel_numerico_we, y_pred_devel_we_mlp_concat,average=average_def)\n",
        "mlp4_we_concatenacion = f1_score(y_test_numerico_we, y_pred_test_we_mlp_concat,average=average_def)\n",
        "\n",
        "parte4_we.append(['MLP Concatenación: ',mlp4_we_concatenacion])\n",
        "\n",
        "#Mostramos los resultados en consola\n",
        "print(\"Resultados en el corpus devel: \")\n",
        "imprimir_resultados(MLP_f1_devel)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGtPM03H_ePL"
      },
      "source": [
        "Observamos:\n",
        "- Resultados\n",
        "  - La macro F1 de MLP utilizando word embeddings tomando la concatenación es aproximadamente 0.56\n",
        "  - La macro F1 de MLP utilizando word embeddings tomando el promedio es aproximadamente 0.58\n",
        "- Concluimos en esta parte que la representación de promedio es mejor tanto en velocidad de procesamiento como en eficacia del análisis de sentimientos, ya que obtiene mejores resultados en tiempos de ejecución menores, aunque el modelo de word embeddings tomando la concatenación tiene capacidad de mejora, ya que sabemos que hay tweets de hasta 22 palabras en nuestro corpus, mientras que solo estamos tomando las 10 primeras que se encuentran identificadas en nuestra colección de vectores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ4HAF2b2lJW"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKQfI_VC8VEj"
      },
      "source": [
        "### Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y91AcNtzTpGM"
      },
      "source": [
        "#### BOW combiando con TF-IDF:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECr1izKLTu1K"
      },
      "source": [
        "Primero realizamos pruebas para determinar los mejores valores de ciertos hiperparámetros a ser utilizados para otener las mejores métricas posibles con este método de aprendizaje automático basado en atributos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "YUvJ7HXF87eZ",
        "outputId": "85d84888-e1a9-480b-9e15-61119cd1c7a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El tiempo requerido para completar la GridSearch es de 1033.73 segundos.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'C': 5, 'kernel': 'linear'}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultado de Cross-Validation (k =5) del mejor estimador: 0.386\n"
          ]
        }
      ],
      "source": [
        "model_svm2 = svm.SVC(random_state=1234)\n",
        "param_dict = {'C': [ 5, 10, 20],\n",
        "             'kernel': ['linear', 'sigmoid']}\n",
        "\n",
        "start = time()\n",
        "grid_search = GridSearchCV(model_svm2, param_dict, scoring='f1_macro')\n",
        "grid_search.fit(X_train_bowP, y_train_mod)\n",
        "print(\"El tiempo requerido para completar la GridSearch es de %.2f segundos.\" % (time()-start))\n",
        "display(grid_search.best_params_)\n",
        "print(\"Resultado de Cross-Validation (k =5) del mejor estimador: %.3f\" % grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-aTxFeBT7SH"
      },
      "source": [
        "Como el mejor estimador proporciona una Macro-F1 de 0.386, declararemos un clasificador en función de dichos valores de hiperparámetros que permitieron alcanzarla luego de aplicar Cross-Validation en el corpus de Entrenamiento, pero tambien probaremos con otros valores debido a que dicho valor de Macro-F1 es bajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "uq3BCbdV8ayp",
        "outputId": "96ccef68-9771-4359-c872-810686b070d5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=5, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=5, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC(C=5, kernel='linear')"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Experimentos con Aprendizaje Atuomático y BoW\n",
        "#SVM\n",
        "model_svm = svm.SVC(C=5, kernel='linear')\n",
        "model_svm.fit(X_train_bowP, y_train_mod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbbwaP-58jx7",
        "outputId": "feeacdd2-ad84-42e5-995a-c49b9f4c4877"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['P', 'N', 'P', 'NONE', 'P', 'N', 'N', 'N', 'P', 'N'], dtype='<U4')"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_svm.predict(X_devel_bowP[:10])    #Mostramos primeras 10 predicciones del Clasificador SVM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wk_Q5p28npD",
        "outputId": "06d1dcc9-326c-4441-9b40-6620acfb3636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro-F1:  0.5730006777836226\n"
          ]
        }
      ],
      "source": [
        "y_pred_devel_BOW_SVM= model_svm.predict(X_devel_bowP)\n",
        "f1_devel_BOW_SVM = f1_score(y_devel, y_pred_devel_BOW_SVM,average=\"macro\")\n",
        "print(\"Macro-F1: \",f1_devel_BOW_SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPRjwYugHcGY"
      },
      "source": [
        "A continuación probaremos el comportamiento del modelo de clasificación SVM para tres valores distintos de C: 3, 8 y 20. Tambien tres funciones de transformación de entrada diferentes: linear, rbf y sigmoid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsPv4akrDDly"
      },
      "outputs": [],
      "source": [
        "def svmBow(ValorC,ValorKernel):\n",
        "  model_svmBowAux = svm.SVC(C=ValorC, kernel=ValorKernel)\n",
        "  model_svmBowAux.fit(X_train_bowP, y_train_mod)\n",
        "  y_pred_devel_BOW_SVM= model_svmBowAux.predict(X_devel_bowP)\n",
        "  f1_devel_BOW_SVM = f1_score(y_devel, y_pred_devel_BOW_SVM,average=\"macro\")\n",
        "  return f1_devel_BOW_SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo85SP2nDanJ",
        "outputId": "eee89a35-b46d-4275-a0cc-b2b6fadc5089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro-F1  para C=3 y kernel=linear:  0.5817227225718874\n",
            "Macro-F1  para C=3 y kernel=rbf:  0.5900978362201837\n",
            "Macro-F1  para C=3 y kernel=sigmoid:  0.5823754773523954\n",
            "Macro-F1  para C=8 y kernel=linear:  0.5614140697110582\n",
            "Macro-F1  para C=8 y kernel=rbf:  0.5910828991322029\n",
            "Macro-F1  para C=8 y kernel=sigmoid:  0.5590214352425277\n",
            "Macro-F1  para C=20 y kernel=linear:  0.5433629548223026\n",
            "Macro-F1  para C=20 y kernel=rbf:  0.5910828991322029\n",
            "Macro-F1  para C=20 y kernel=sigmoid:  0.5461235453515433\n"
          ]
        }
      ],
      "source": [
        "print(\"Macro-F1  para C=3 y kernel=linear: \",svmBow(3,\"linear\"))\n",
        "print(\"Macro-F1  para C=3 y kernel=rbf: \",svmBow(3,\"rbf\"))\n",
        "print(\"Macro-F1  para C=3 y kernel=sigmoid: \",svmBow(3,\"sigmoid\"))\n",
        "print(\"Macro-F1  para C=8 y kernel=linear: \",svmBow(8,\"linear\"))\n",
        "print(\"Macro-F1  para C=8 y kernel=rbf: \",svmBow(8,\"rbf\"))\n",
        "print(\"Macro-F1  para C=8 y kernel=sigmoid: \",svmBow(8,\"sigmoid\"))\n",
        "print(\"Macro-F1  para C=20 y kernel=linear: \",svmBow(20,\"linear\"))\n",
        "print(\"Macro-F1  para C=20 y kernel=rbf: \",svmBow(20,\"rbf\"))\n",
        "print(\"Macro-F1  para C=20 y kernel=sigmoid: \",svmBow(20,\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akvEqr1-IE44"
      },
      "source": [
        "De la anterior salida, vemos que el valor de kernel = \"rbf\" arroja los mejores resultados, para distintos valores de C. Por lo tanto veremos si es aun posible mejorar tales performance, al incluir el hiperparámetro gamma que es un hiperparámetro de ajuste que controla la influencia de cada muestra en el modelo. Dicho valor suele combinarse con \"rbf\". Dada la salida anterior solo consideraremos C = 3 ya que arrojo buenos resultados en general."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTAxCTCyXi97"
      },
      "outputs": [],
      "source": [
        "def svmBowGamma(ValorC,ValorKernel,Gamma):\n",
        "  model_svmBowAux = svm.SVC(C=ValorC, kernel=ValorKernel,gamma=Gamma)\n",
        "  model_svmBowAux.fit(X_train_bowP, y_train_mod)\n",
        "  y_pred_devel_BOW_SVM= model_svmBowAux.predict(X_devel_bowP)\n",
        "  f1_devel_BOW_SVM = f1_score(y_devel, y_pred_devel_BOW_SVM,average=\"macro\")\n",
        "  return f1_devel_BOW_SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10fG3p77XxAU",
        "outputId": "800385ad-7bbb-42c2-b861-429616a553be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro-F1  para C=3, kernel=rbf y gamma=0.1:  0.5965084127696065\n",
            "Macro-F1  para C=3, kernel=rbf y gamma=1:  0.5880618662434957\n",
            "Macro-F1  para C=3, kernel=rbf y gamma=2:  0.5976970560303894\n",
            "Macro-F1  para C=3, kernel=rbf y gamma=5:  0.2978999398781148\n"
          ]
        }
      ],
      "source": [
        "print(\"Macro-F1  para C=3, kernel=rbf y gamma=0.1: \",svmBowGamma(3,\"rbf\",0.1))\n",
        "print(\"Macro-F1  para C=3, kernel=rbf y gamma=1: \",svmBowGamma(3,\"rbf\",1))\n",
        "print(\"Macro-F1  para C=3, kernel=rbf y gamma=2: \",svmBowGamma(3,\"rbf\",2))\n",
        "print(\"Macro-F1  para C=3, kernel=rbf y gamma=5: \",svmBowGamma(3,\"rbf\",5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdOesBnSZjYY"
      },
      "source": [
        "Se obtienen leves mejoras respecto a la ejecución anterior al considerar distintos valores de gamma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyTXsPCt-wuL"
      },
      "source": [
        "Como el uso de BOWs combinados con Tf-Idf para representar los tweets en un modelo SVM con C = 3, kernel = \"rbf\" y gamma = 2 es uno de los enfoques que ha logrado mejores resultados, ejecutamos nuevamente para determinar la F1 de cada clase:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wk0Olg__Tgv",
        "outputId": "5db6453d-02f7-4e69-f0f3-c58117026602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 (P):    0.6296296296296297\n",
            "F1 (N):    0.5\n",
            "F1 (NONE): 0.6634615384615384\n",
            "Macro-F1: 0.5976970560303894\n"
          ]
        }
      ],
      "source": [
        "model_svmBowCandidato = svm.SVC(C=3, kernel= \"rbf\",gamma= 2)\n",
        "model_svmBowCandidato.fit(X_train_bowP, y_train_mod)\n",
        "y_pred_devel_BOW_SVM_C= model_svmBowCandidato.predict(X_devel_bowP)\n",
        "f1_devel_BOW_SVM_Cand = f1_score(y_devel, y_pred_devel_BOW_SVM_C,average=None)\n",
        "imprimir_resultados(f1_devel_BOW_SVM_Cand)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGAG-DYLT6OP"
      },
      "source": [
        "Estos resultados seran mencionados en la pregunta 7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMe79u31VIn6"
      },
      "source": [
        "#### BOW para k-features mas relevantes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-GtkB6UVNop"
      },
      "source": [
        "Al igual que como se hizo con MLP, agregamos dos coordendas extras. Como anteriormente se obtuvieron los mejores resultados para kernel = \"rbf\", consideramos a continuación dicha función de transformación. A su vez probamos el comportamiento para distintos valores de k."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3a30xXvRvru"
      },
      "outputs": [],
      "source": [
        "def svmBowK_ext(ValorC,ValorKernel,ValorK):\n",
        "  vocabularioSVM = seleccionarK(X_trainP,y_train_mod,ValorK)\n",
        "  X_train_bow_k_svm = representar_con_k(X_trainP,vocabularioSVM)\n",
        "  X_train_bow_k_svm_extendido = agregar_a_bow(X_train_bow_k_svm,extras)     # Agregamos dos coordenadas extras a BOWs de train\n",
        "\n",
        "  model_svmBowAux = svm.SVC(C=ValorC, kernel=ValorKernel)\n",
        "  model_svmBowAux.fit(X_train_bow_k_svm_extendido, y_train_mod)\n",
        "\n",
        "  X_devel_bow_k_svm = representar_con_k(X_develP,vocabularioSVM)\n",
        "  X_devel_bow_k_svm_extendido = agregar_a_bow(X_devel_bow_k_svm,extras2)    # Agregamos dos coordenadas extras a BOWs de devel\n",
        "\n",
        "  y_pred_devel_BOW_K_SVM= model_svmBowAux.predict(X_devel_bow_k_svm_extendido)\n",
        "  f1_devel_BOW_K_SVM = f1_score(y_devel, y_pred_devel_BOW_K_SVM,average=\"macro\")\n",
        "  return f1_devel_BOW_K_SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvsc2nV8SdB2",
        "outputId": "6c31467a-621f-40f3-8bf4-e24c80dd64f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro-F1  para C=3, kernel=rbf y k=250:  0.5145076263796833\n",
            "Macro-F1  para C=3, kernel=rbf y k=500:  0.5304865230779972\n",
            "Macro-F1  para C=3, kernel=rbf y k=1000:  0.5285714630418378\n",
            "Macro-F1  para C=8, kernel=rbf y k=250:  0.5144459242299119\n",
            "Macro-F1  para C=8, kernel=rbf y k=500:  0.5232950859348796\n",
            "Macro-F1  para C=8, kernel=rbf y k=1000:  0.5214419252393935\n",
            "Macro-F1  para C=20, kernel=rbf y k=250:  0.5144459242299119\n",
            "Macro-F1  para C=20, kernel=rbf y k=500:  0.5223809368786031\n",
            "Macro-F1  para C=20, kernel=rbf y k=1000:  0.5175840110865974\n"
          ]
        }
      ],
      "source": [
        "print(\"Macro-F1  para C=3, kernel=rbf y k=250: \",svmBowK_ext(3,\"rbf\",250))\n",
        "print(\"Macro-F1  para C=3, kernel=rbf y k=500: \",svmBowK_ext(3,\"rbf\",500))\n",
        "print(\"Macro-F1  para C=3, kernel=rbf y k=1000: \",svmBowK_ext(3,\"rbf\",1000))\n",
        "print(\"Macro-F1  para C=8, kernel=rbf y k=250: \",svmBowK_ext(8,\"rbf\",250))\n",
        "print(\"Macro-F1  para C=8, kernel=rbf y k=500: \",svmBowK_ext(8,\"rbf\",500))\n",
        "print(\"Macro-F1  para C=8, kernel=rbf y k=1000: \",svmBowK_ext(8,\"rbf\",1000))\n",
        "print(\"Macro-F1  para C=20, kernel=rbf y k=250: \",svmBowK_ext(20,\"rbf\",250))\n",
        "print(\"Macro-F1  para C=20, kernel=rbf y k=500: \",svmBowK_ext(20,\"rbf\",500))\n",
        "print(\"Macro-F1  para C=20, kernel=rbf y k=1000: \",svmBowK_ext(20,\"rbf\",1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69kVFLxJX5l4"
      },
      "source": [
        "Analizando la salida vemos que los mejores resultados se corresponden cuando k = 500, con C = 3, donde recordemos los BOWs de de los tweets tenian dos coordenadas extras que contemplaban la cantidad de palabras positivas en la anteúltima coordenada, y negativas en la última."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnvtwOdXZzbR"
      },
      "source": [
        "Por otro lado, si utilizamos kernel = \"linear\", a diferencia del enfoque de BOWs combinados con TF-IDF, tenemos que \"linear\" arroja mejores resultados que \"rbf\", considerando k = 500 que fue el de mejor rendimiento en las celdas anterores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSvDB7xEZDEM",
        "outputId": "618608b0-0a95-4d20-bc12-dad1592f192d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro-F1  para C=3, kernel=linear y k=500:  0.5492594510453775\n",
            "Macro-F1  para C=8, kernel=linear y k=500:  0.5429893572237808\n",
            "Macro-F1  para C=20, kernel=linear y k=500:  0.5387224285971939\n"
          ]
        }
      ],
      "source": [
        "print(\"Macro-F1  para C=3, kernel=linear y k=500: \",svmBowK_ext(3,\"linear\",500))\n",
        "print(\"Macro-F1  para C=8, kernel=linear y k=500: \",svmBowK_ext(8,\"linear\",500))\n",
        "print(\"Macro-F1  para C=20, kernel=linear y k=500: \",svmBowK_ext(20,\"linear\",500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm4aztp5YUoN"
      },
      "source": [
        "Por último veamos que sucede si no agregamos las dos coordendas extras, con k = 500, C = 5 y kernel = \"linear\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6cKPCORYZ8z",
        "outputId": "578605f5-8e6c-4950-f112-45fc3fb85388"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro-F1  para C=3, kernel=linear y k=500:  0.5330609615939691\n"
          ]
        }
      ],
      "source": [
        "model_svm_sin_ext = svm.SVC(C=3, kernel='linear')\n",
        "vocabularioSVM_sin_ext = seleccionarK(X_trainP,y_train_mod,2000)\n",
        "\n",
        "X_train_bow_k_svm_sin_ext = representar_con_k(X_trainP,vocabularioSVM_sin_ext)\n",
        "\n",
        "model_svm_sin_ext.fit(X_train_bow_k_svm_sin_ext, y_train_mod)\n",
        "\n",
        "X_devel_bow_k_svm3 = representar_con_k(X_develP,vocabularioSVM_sin_ext)\n",
        "\n",
        "y_pred_devel_BOW_K_SVM3 = model_svm_sin_ext.predict(X_devel_bow_k_svm3)\n",
        "f1_devel_BOW_K_SVM3 = f1_score(y_devel, y_pred_devel_BOW_K_SVM3,average=\"macro\")\n",
        "print(\"Macro-F1  para C=3, kernel=linear y k=500: \",f1_devel_BOW_K_SVM3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITFDWzJtbUw7"
      },
      "source": [
        "Vemos que el rendimiento es practicamente igual al obtenido con C = 3, kernel = \"rbf\", k = 500 y agregando las dos coordenadas. Entonces, en un principio, el uso de las mismas no implica primariamente mejores resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yExY-y8fxEm_"
      },
      "source": [
        "En resumen, SVM trabaja mejor con BOWs combinados con Tf-Idf, aunque dicha representación requiere mayor espacio de almacenamiento debido a la gran diferencia entre las dimensiones de los vectores. Por lo tanto en la práctica habría que analizar si dicho costo se puede asumir en busca de mejores resultados. Sin embargo, se suele optar por redes neuronales basadas en la arquitectura transformers como veremeos en la parte 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENhvKIa0A5pT"
      },
      "source": [
        "### Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giO_as4M5pBh"
      },
      "source": [
        "#### Word embeddings promedio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pExgUhnFBQjy"
      },
      "source": [
        "Para esta parte se probó:\n",
        "- En una primera instancia con varios valores distintos para el parámetro 'C', obteniendo como valores altos como los más apropiados.\n",
        "- Luego, se verificó para varios tipos de núcleo ajustando el parámetro 'kernel' como 'linear' o 'sigmoide', así como gaussiano ('rbf') ajustando el parámetro gamma y polinomial ('poly') ajustando el parámetro 'degree'.\n",
        "- Finalmente probamos variando tanto el corpus de entrada (train puro o train con los léxicos concatenados) y la variable class_weight ('balanced' o por defecto).\\\n",
        "De los resultados anteriores, presentamos el mejor clasificador obtenido:\\\n",
        "Para corpus train puro, es decir, sin léxicos agregados como tweets, con los parámetros C=70, kernel = 'linear' y class_weight balanceado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "odmbyZLdPkd-",
        "outputId": "32614d18-d956-4356-fb78-0332a0022e8f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=30, class_weight=&#x27;balanced&#x27;, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=30, class_weight=&#x27;balanced&#x27;, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC(C=30, class_weight='balanced', kernel='linear')"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "######################################################\n",
        "################ SVM con Word embeddings:#############\n",
        "######################################################\n",
        "\n",
        "svm_we_prom = svm.SVC(C=63, kernel='linear',class_weight='balanced')\n",
        "svm_we_prom.fit(X_train_we_promedio, y_train_numerico_we)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIRRM06yA5pV",
        "outputId": "9e2641f3-9d4f-4fee-bf46-5634d0b500f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy en c-v de 5 etapas:[0.60853879 0.60012026 0.61395069 0.5694528  0.59386282]\n"
          ]
        }
      ],
      "source": [
        "# cross-validation de 5 etapas:\n",
        "#Solo utilizamos el corpus train, sin agregar el léxico.\n",
        "\n",
        "model_svm_acc = cross_val_score(estimator=svm_we_prom, X=X_train_we_promedio, y=y_train_numerico_we, cv=5, n_jobs=-1)\n",
        "print('Accuracy en c-v de 5 etapas:' + str(model_svm_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJGjWQEwA5pV",
        "outputId": "c515f66b-0452-49d3-d4c3-957c142880a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultados en el corpus devel: \n",
            "F1 (P):    0.6536964980544746\n",
            "F1 (N):    0.625514403292181\n",
            "F1 (NONE): 0.5130890052356021\n",
            "Macro-F1: 0.5974333021940859\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################################################\n",
        "#####################  Cálculo de medida F1  #####################\n",
        "##################################################################\n",
        "\n",
        "y_pred_devel_svm_we = svm_we_prom.predict(X_devel_we_promedio)\n",
        "svm_f1_devel = f1_score(y_devel_numerico_we, y_pred_devel_svm_we, average=None)\n",
        "\n",
        "print(\"Resultados en el corpus devel: \")\n",
        "imprimir_resultados(svm_f1_devel)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXvpeECB5te4"
      },
      "source": [
        "#### Word embeddings concatenación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "J1SauaP39oHg",
        "outputId": "8e98f7f5-076e-48e3-a4b1-212b1d8f4b6c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=22, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=22, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC(C=22, kernel='linear')"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svm_we_concatenacion = SVC(C=22, kernel='linear')\n",
        "svm_we_concatenacion.fit(X_train_we_concatenacion, y_train_numerico_we)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3ybt7Ta-fkn",
        "outputId": "a636977a-b7b6-4fd2-d90c-1de0f306efe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultados en el corpus devel: \n",
            "F1 (P):    0.5775656324582339\n",
            "F1 (N):    0.5142002989536623\n",
            "F1 (NONE): 0.47556142668428003\n",
            "Macro-F1: 0.5224424526987254\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################################################\n",
        "#####################  Cálculo de medida F1  #####################\n",
        "##################################################################\n",
        "\n",
        "y_pred_devel_svm_we = svm_we_concatenacion.predict(X_devel_we_concatenacion)\n",
        "y_pred_test_svm_we = svm_we_concatenacion.predict(X_test_we_concatenacion)\n",
        "\n",
        "average_def = None\n",
        "\n",
        "svm_f1_devel = f1_score(y_devel_numerico_we, y_pred_devel_svm_we, average=average_def)\n",
        "\n",
        "print(\"Resultados en el corpus devel: \")\n",
        "imprimir_resultados(svm_f1_devel)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa6uwGRkKuQ5"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RoVdiXyKuQ6"
      },
      "source": [
        "A modo de experimentación, veremos como se comporta el modelo de aprendizaje automático basado en atributos Naive Bayes para BOWs combinados con Tf-Idf. No lo haremos con WE ya que estos son representaciones vectoriales densas de palabras que capturan el significado semántico y la relación entre las palabras. Mientras que normalmente  Naive Bayes se utiliza con características basadas en recuentos de palabras, como la frecuencia de términos o la presencia/ausencia de palabras específicas en un documento. Es decir, es mas conveniente para representaciones basadas en BOW. Por otro lado los resultados no son alentadores como para tenerlo mas en consideración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh9MpwdqKuQ6",
        "outputId": "af4d13eb-891d-43b8-d2cf-1f92fc636248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 (P):    0.4714285714285714\n",
            "F1 (N):    0.2576271186440678\n",
            "F1 (NONE): 0.5215605749486653\n",
            "Macro-F1: 0.4168720883404348\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "model_gnb = GaussianNB()\n",
        "model_gnb.fit(X_train_bowP.toarray(), y_train_mod)\n",
        "y_predict_NB = model_gnb.predict(X_devel_bowP.toarray())\n",
        "res_NB = f1_score(y_devel, y_predict_NB, average=None)\n",
        "imprimir_resultados(res_NB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8N7inTNKuQ7"
      },
      "source": [
        "## Regresión Logística"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7sZbGdKMR5e"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7ewZ0FhKuQ7"
      },
      "source": [
        "Al igual que con Naive Bayes, armamos un simple clasificador para este modelo para experimentar como responde a esta tarea de PLN. Consideramos utilizar los tweets en representacion BOW, y también en WE Promedio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dIngCyjNL6-"
      },
      "source": [
        "### Bag Of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4DwdVqqKuQ7",
        "outputId": "afeba0ea-7848-467b-9284-d2c2939664d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 (P):    0.6048387096774194\n",
            "F1 (N):    0.466765140324963\n",
            "F1 (NONE): 0.6548042704626335\n",
            "Macro-F1: 0.5754693734883386\n"
          ]
        }
      ],
      "source": [
        "model_lg = LogisticRegression()\n",
        "model_lg.fit(X_train_bowP.toarray(), y_train_mod)\n",
        "y_predict_LG = model_lg.predict(X_devel_bowP.toarray())\n",
        "res_LG = f1_score(y_devel, y_predict_LG, average=None)\n",
        "imprimir_resultados(res_LG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "111v4cl-LllO"
      },
      "source": [
        "### Word embeddings Promedio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSkMmg6aLnMP",
        "outputId": "429d7cc4-bbad-4a9a-a5a2-fafa72b18c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 (P):    0.6626065773447015\n",
            "F1 (N):    0.6100278551532033\n",
            "F1 (NONE): 0.5324137931034483\n",
            "Macro-F1: 0.6016827418671178\n"
          ]
        }
      ],
      "source": [
        "model_lg_we = LogisticRegression(C=0.1,max_iter=1000)\n",
        "model_lg_we.fit(X_train_we_promedio, y_train_numerico_we)\n",
        "y_predict_LG = model_lg_we.predict(X_devel_we_promedio)\n",
        "res_LG = f1_score(y_devel_numerico_we, y_predict_LG, average=None)\n",
        "imprimir_resultados(res_LG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_isiIcbNeEL"
      },
      "source": [
        "Como podemos ver, los resultados de esta parte son bastante buenos. En BoW la clase que obtiene peor F1 es \"N\", y la mejor \"NONE\". Por otro lado, en WE obtenemos un peor F1 para la clase NONE, mientras que los de clase P y N tienen resultados ampliamente superiores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubvGm7-b2eQN"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y60rrXhaJ0s"
      },
      "source": [
        "Imports necesarios para esta parte:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT4aXETTecXe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout, Embedding\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import GlobalMaxPool2D, BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1uuEk1Okw5k",
        "outputId": "0ab149f2-78ab-4b6a-e665-3d53a4d4053b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0lA0uCFktf3",
        "outputId": "2751d402-80bf-4dad-9182-128fadc3f983"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_addons as tfa\n",
        "f1_macro = tfa.metrics.F1Score(num_classes=3, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42e5Lz6baBe8"
      },
      "source": [
        "### LSTM sin atributos en los Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY50eaCWeKGo"
      },
      "source": [
        "Primero definimos una función que determina el largo mas grande de un tweet en un corpus (procesado):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13RYJF3UeXvN",
        "outputId": "01a224a8-2ab4-430b-b3a4-171cd8c7e1f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21\n",
            "18\n"
          ]
        }
      ],
      "source": [
        "def largo_maximo(corpus):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  sequence_lengths = [len(sequence.split()) for sequence in corpus] # Obtenemos las longitudes de las secuencias\n",
        "  max_length = max(sequence_lengths)\n",
        "  return max_length\n",
        "\n",
        "max_length_train = largo_maximo(X_trainP)\n",
        "max_length_devel = largo_maximo(X_develP)\n",
        "print(max_length_train)\n",
        "print(max_length_devel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpnkhQRKhVe8"
      },
      "source": [
        "A continuacion, obtenemos una matriz con los WordEmbeddings de las palabras que aparecen en el Corpus de entrenamiento. De esta forma aprovechamos WE pre-entrenados en una capa Embedding de la red LSTM que definiremos proximamente. Tambien llevamos a los conjuntos de datos de entrenamiento y desarrollo al formato adecuado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDKxDgS_hNyn"
      },
      "outputs": [],
      "source": [
        "embedding_model = Cool.copy()                            # Cool obtenido en descraga de WE\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_trainP)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_trainP)   # Convertimos los conjuntos en secuencias de índices\n",
        "X_devel_sequences = tokenizer.texts_to_sequences(X_develP)\n",
        "\n",
        "max_length = max_length_train                                # Ajustamos las secuencias a una longitud máxima, la del Data set de entrenamiento\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length)\n",
        "X_devel_padded = pad_sequences(X_devel_sequences, maxlen=max_length)\n",
        "\n",
        "embedding_dim = 300                                          # Creamos una matriz de embedding inicializada con ceros\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():                 # Rellenamos la matriz de embedding con los vectores pre-entrenados\n",
        "    if word in embedding_model:\n",
        "        embedding_matrix[i] = embedding_model[word]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CZLA7K9jmXY"
      },
      "source": [
        "Llevamos los conjuntos de etiquetas de polaridad a un formato One-Hot ya que trabajamos con 3 tipos diferentes de las mismas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2R57ogIjmqm"
      },
      "outputs": [],
      "source": [
        "sentimientos = ['N', 'P', 'NONE']\n",
        "\n",
        "def etiquetas_one_hot(etiquetas_originaless):\n",
        "  etiquetas_esperadas = np.zeros((len(etiquetas_originaless), len(sentimientos)))\n",
        "  for i, etiqueta in enumerate(etiquetas_originaless):\n",
        "      index = sentimientos.index(etiqueta)\n",
        "      etiquetas_esperadas[i, index] = 1\n",
        "  return etiquetas_esperadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzEcLPOjkK34"
      },
      "outputs": [],
      "source": [
        "y_trainHot = etiquetas_one_hot(y_train_mod)\n",
        "y_develHot = etiquetas_one_hot(y_devel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXme-VjFkeRM"
      },
      "source": [
        "Definimos la primer red LSTM. La misma utiliza los WE pre-entrenados que fueron cargados en Parte 2 y puestos en una matriz anteriormente. En el resumen de la Red, veremos que esta dispone para entrenar mas de 85.000 parámetros, contando con otros 6.600.000 de parámetros aproximadamente no entrenables, pues quedaron determinados por los WE pre-entrenados.  Utilizamos una Capa Bidireccional LSTM para capturar tanto el contexto pasado como el futuro de las secuencias.\n",
        "\n",
        "\n",
        "Se compila el modelo utilizando el optimizador Adam, la función de pérdida categorical_crossentropy y se utiliza la métrica F1 Score para evaluar el rendimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUIknuoDkisn"
      },
      "outputs": [],
      "source": [
        "def lstm1 (Max_length, X_trainAux, y_trainAux, X_develAux, y_develAux,Mat):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, 300, weights=[Mat],\n",
        "                      input_length=Max_length, trainable=False))\n",
        "  model.add(Bidirectional(LSTM(32)))\n",
        "  model.add(Dense(3, activation='softmax'))      # 3 por las 3 etiquetas posibles, con activation \"softmax\" que es la adecuada para tal caso\n",
        "  # Compilamos el modelo, usamos loss = 'categorical_crossentropy' ya que tenemos tres categorias diferentes\n",
        "  model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=[tfa.metrics.F1Score(average='macro',num_classes=3)], run_eagerly=True)\n",
        "  model.summary()\n",
        "  model.fit(X_trainAux, y_trainAux, epochs=5, batch_size=32)          # Entrenamos el modelo\n",
        "  y_obtenido = model.predict(X_develAux)\n",
        "  f1_macro = tfa.metrics.F1Score(num_classes=3, average='macro')\n",
        "  f1_macro.update_state(y_develAux, y_obtenido)\n",
        "  macro_f1Aux2 = f1_macro.result().numpy()\n",
        "  return macro_f1Aux2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJ790kGHX-UF",
        "outputId": "de5333d2-a936-452a-d416-cc63f03a6091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_15 (Embedding)    (None, 21, 300)           6531600   \n",
            "                                                                 \n",
            " bidirectional_14 (Bidirecti  (None, 64)               85248     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,617,043\n",
            "Trainable params: 85,443\n",
            "Non-trainable params: 6,531,600\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "575/575 [==============================] - 124s 214ms/step - loss: 0.9245 - f1_score: 0.5566\n",
            "Epoch 2/5\n",
            "575/575 [==============================] - 123s 215ms/step - loss: 0.6879 - f1_score: 0.7209\n",
            "Epoch 3/5\n",
            "575/575 [==============================] - 129s 224ms/step - loss: 0.5699 - f1_score: 0.7692\n",
            "Epoch 4/5\n",
            "575/575 [==============================] - 137s 238ms/step - loss: 0.5351 - f1_score: 0.7830\n",
            "Epoch 5/5\n",
            "575/575 [==============================] - 136s 236ms/step - loss: 0.5136 - f1_score: 0.7906\n",
            "36/36 [==============================] - 4s 110ms/step\n",
            "0.59127665\n"
          ]
        }
      ],
      "source": [
        "macro_lstm1 = lstm1 (21,X_train_padded,y_trainHot,X_devel_padded,y_develHot,embedding_matrix)\n",
        "print(macro_lstm1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKzUEhggm91k"
      },
      "source": [
        "Como podemos observar, luego de 5 épocas se alcanza una Macro-F1 de 0.591 aproximadamente. Por otro lado, vemos también que los tiempos requeridos para el entrenamiento no son malos, pues se requiere aproximadamente 2 minutos por época."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xecRzr26nl_t"
      },
      "source": [
        "Pasamos a definir una nueva red LSTM. Volvemos a entrenar durante 5 epocas. Respecto al modelo anterior, pasamos a utilizar capas más variadas en busca de mejorar la performance. Entre dichas capas, tenemos dos capas densas (Dense) con activación ReLU para aprender representaciones más abstractas de los datos, una capa de convolución unidimensional y capas de SpatialDropout y Dropout que son incluidas para regularizar los datos y reducir el sobreajuste.\n",
        "\n",
        "Ademas se define un callback ReduceLROnPlateau2 para reducir la tasa de aprendizaje cuando la pérdida en el conjunto de validación deja de mejorar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfb9gIsMnXRC"
      },
      "outputs": [],
      "source": [
        "# Max_length se refiere a la secuencia de mayor largo en los corpus, mientras que Mat es la matriz de WE pre-entrenados\n",
        "def lstm2 (Max_length, X_trainAux, y_trainAux, X_develAux, y_develAux,Mat):\n",
        "  LR = 0.001\n",
        "  embedding_layer = tf.keras.layers.Embedding(vocab_size,300,weights=[Mat],\n",
        "                                          input_length=Max_length,trainable=False)\n",
        "  sequence_input = Input(shape=(Max_length,), dtype='int32')      # Definimos condiciones de entradas que pasaran a la capa de Embedding\n",
        "  embedding_sequences = embedding_layer(sequence_input)\n",
        "  x = SpatialDropout1D(0.2)(embedding_sequences)                  # Regulariza los datos y reduce el sobreajuste.\n",
        "  x = Conv1D(64, 5, activation='relu')(x)                         # Capa de convolución unidimensional\n",
        "  x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  outputs = Dense(3, activation='softmax')(x)\n",
        "  model = tf.keras.Model(sequence_input , outputs)\n",
        "  model.compile(optimizer=Adam(learning_rate=LR), loss = 'categorical_crossentropy', metrics=[tfa.metrics.F1Score(average='macro',num_classes=3)])\n",
        "  model.summary()\n",
        "  ReduceLROnPlateau2 = ReduceLROnPlateau(factor=0.1,min_lr = 0.01, monitor = 'val_loss',verbose = 1)\n",
        "  training = model.fit(X_trainAux, y_trainAux, batch_size=1024, epochs=5,\n",
        "                    validation_data=(X_develAux, y_develAux), callbacks=[ReduceLROnPlateau2])\n",
        "  scores = model.predict(X_develAux, verbose=1, batch_size=10000)\n",
        "  f1_macro2 = tfa.metrics.F1Score(num_classes=3, average='macro')\n",
        "  f1_macro2.update_state(y_develAux, scores)\n",
        "  macro_f1Aux3 = f1_macro2.result().numpy()\n",
        "  return macro_f1Aux3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97L4houX8VhM",
        "outputId": "896bd5d3-4fb4-44c3-92b9-12bc80514bb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 21)]              0         \n",
            "                                                                 \n",
            " embedding_13 (Embedding)    (None, 21, 300)           6531600   \n",
            "                                                                 \n",
            " spatial_dropout1d_12 (Spati  (None, 21, 300)          0         \n",
            " alDropout1D)                                                    \n",
            "                                                                 \n",
            " conv1d_16 (Conv1D)          (None, 17, 64)            96064     \n",
            "                                                                 \n",
            " bidirectional_12 (Bidirecti  (None, 128)              66048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 512)               66048     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,023,955\n",
            "Trainable params: 492,355\n",
            "Non-trainable params: 6,531,600\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "18/18 [==============================] - 23s 887ms/step - loss: 1.0720 - f1_score: 0.4402 - val_loss: 0.9944 - val_f1_score: 0.5053 - lr: 0.0010\n",
            "Epoch 2/5\n",
            "18/18 [==============================] - 15s 866ms/step - loss: 0.8498 - f1_score: 0.6697 - val_loss: 0.9810 - val_f1_score: 0.5390 - lr: 0.0010\n",
            "Epoch 3/5\n",
            "18/18 [==============================] - 15s 864ms/step - loss: 0.6382 - f1_score: 0.7329 - val_loss: 0.9824 - val_f1_score: 0.5284 - lr: 0.0010\n",
            "Epoch 4/5\n",
            "18/18 [==============================] - 15s 862ms/step - loss: 0.5878 - f1_score: 0.7489 - val_loss: 0.9776 - val_f1_score: 0.5757 - lr: 0.0010\n",
            "Epoch 5/5\n",
            "18/18 [==============================] - 16s 873ms/step - loss: 0.5666 - f1_score: 0.7608 - val_loss: 0.9346 - val_f1_score: 0.5806 - lr: 0.0010\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "0.58064616\n",
            "()\n",
            "0.58064616\n"
          ]
        }
      ],
      "source": [
        "macro_lstm2 = lstm2 (21,X_train_padded,y_trainHot,X_devel_padded,y_develHot,embedding_matrix)\n",
        "print(macro_lstm2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jiVJHz0o6-R"
      },
      "source": [
        "Vemos que la Macro-F1 obtenida es levemente menor respecto a la del modelo anterior, pero ejecutando este modelo de forma mucho mas rapida y entrenando prácticamente 5 veces mas la cantidad de parámetros respecto al anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D058nvGZeKVZ"
      },
      "source": [
        "--------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q-nAzuXpjLF"
      },
      "source": [
        "### LSTM con atributos en los Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkN26gJ4kmQP",
        "outputId": "3e8432cf-200f-4629-a445-e932729264b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Las palabras mas comunes del corpus son: ['no', 'pero', 'si', 'mas', 'hoy', 'gracias', 'muy', 'dia', 'ana', 'ue']\n",
            "Nuestra colección tiene 20030 vectores cargados\n"
          ]
        }
      ],
      "source": [
        "#Cargamos los embeddings incluyendo los léxicos:\n",
        "coleccion_vectores_we_lexicos = cargar_top_embeddings(wordvectors,0,Train_procesado,True)\n",
        "embedding_model2 = coleccion_vectores_we_lexicos.copy()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_trainP)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X_train_sequences_lexicos = tokenizer.texts_to_sequences(X_trainP)   # Convertimos los conjuntos en secuencias de índices\n",
        "X_devel_sequences_lexicos = tokenizer.texts_to_sequences(X_develP)\n",
        "\n",
        "max_length = max_length_train                                # Ajustamos las secuencias a una longitud máxima, la del Data set de entrenamiento\n",
        "X_train_padded_lexicos = pad_sequences(X_train_sequences_lexicos, maxlen=max_length)\n",
        "X_devel_padded_lexicos = pad_sequences(X_devel_sequences_lexicos, maxlen=max_length)\n",
        "\n",
        "embedding_dim = 302                                          # Creamos una matriz de embedding inicializada con ceros\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():                 # Rellenamos la matriz de embedding con los vectores pre-entrenados\n",
        "    if word in embedding_model2:\n",
        "        embedding_matrix[i] = embedding_model2[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ypd2aEUkyOI"
      },
      "outputs": [],
      "source": [
        "y_trainHot = etiquetas_one_hot(y_train_mod)\n",
        "y_develHot = etiquetas_one_hot(y_devel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsGf6Id8k1a2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def lstm3 (Max_length, X_trainAux, y_trainAux, X_develAux, y_develAux,Mat,nro_epocas):\n",
        "  model = Sequential()\n",
        "                                #Esta vez seteamos en 302 la dimension de los vectores en la entrada.\n",
        "  model.add(Embedding(vocab_size, 302, weights=[Mat],\n",
        "                      input_length=Max_length, trainable=False))\n",
        "  model.add(Bidirectional(LSTM(64)))\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=[tfa.metrics.F1Score(average='macro',num_classes=3)], run_eagerly=True)\n",
        "  model.summary()\n",
        "\n",
        "  for i in range (1,nro_epocas):\n",
        "    model.fit(X_trainAux, y_trainAux, epochs=1, batch_size=32)\n",
        "    scores = model.predict(X_develAux, verbose=1, batch_size=10000)\n",
        "    f1_macro2 = tfa.metrics.F1Score(num_classes=3, average=None)\n",
        "    f1_macro2.update_state(y_develAux, scores)\n",
        "    imprimir_resultados((f1_macro2.result().numpy()))\n",
        "  return f1_macro2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIEodFun8pjA"
      },
      "source": [
        "Al entrenar este modelo durante 10 épocas con batch_size = 32, obtuvimos el siguiente resultado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXt0IbopqYxn",
        "outputId": "69d0b8f0-d99b-4779-a57a-aa0dc45e6abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 21, 302)           6575144   \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 128)              187904    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,763,435\n",
            "Trainable params: 188,291\n",
            "Non-trainable params: 6,575,144\n",
            "_________________________________________________________________\n",
            "575/575 [==============================] - 173s 301ms/step - loss: 0.6069 - f1_score: 0.7339\n",
            "1/1 [==============================] - 1s 528ms/step\n",
            "F1 (P):    0.46777165\n",
            "F1 (N):    0.63183475\n",
            "F1 (NONE): 0.4832962\n",
            "Macro-F1: 0.5276341835657755\n",
            "575/575 [==============================] - 173s 301ms/step - loss: 0.4585 - f1_score: 0.7949\n",
            "1/1 [==============================] - 1s 513ms/step\n",
            "F1 (P):    0.6086956\n",
            "F1 (N):    0.6560726\n",
            "F1 (NONE): 0.4617605\n",
            "Macro-F1: 0.5755095879236857\n",
            "575/575 [==============================] - 178s 310ms/step - loss: 0.4324 - f1_score: 0.8077\n",
            "1/1 [==============================] - 1s 590ms/step\n",
            "F1 (P):    0.6223958\n",
            "F1 (N):    0.65891474\n",
            "F1 (NONE): 0.5041551\n",
            "Macro-F1: 0.5951552391052246\n",
            "575/575 [==============================] - 178s 310ms/step - loss: 0.4127 - f1_score: 0.8223\n",
            "1/1 [==============================] - 1s 819ms/step\n",
            "F1 (P):    0.6101231\n",
            "F1 (N):    0.63863343\n",
            "F1 (NONE): 0.515544\n",
            "Macro-F1: 0.5881001949310303\n",
            "575/575 [==============================] - 180s 312ms/step - loss: 0.3972 - f1_score: 0.8284\n",
            "1/1 [==============================] - 1s 504ms/step\n",
            "F1 (P):    0.6272618\n",
            "F1 (N):    0.66490066\n",
            "F1 (NONE): 0.5088235\n",
            "Macro-F1: 0.6003286838531494\n"
          ]
        }
      ],
      "source": [
        "macro_lstm3 = lstm3(21,X_train_padded_lexicos,y_trainHot,X_devel_padded_lexicos,y_develHot,embedding_matrix,6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6VGFrEoWHr3"
      },
      "source": [
        "## Parte 4: Evaluación sobre test\n",
        "\n",
        "Deben probar los mejores modelos obtenidos en la parte anterior sobre el corpus de test.\n",
        "\n",
        "También deben comparar sus resultados con un modelo pre-entrenado para análisis de sentimientos de la biblioteca [pysentimiento](https://github.com/pysentimiento/pysentimiento) (deben aplicarlo sobre el corpus de test).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKfNPvrnBRmK"
      },
      "source": [
        "## Evaluación sobre Test\n",
        "Los dos modelos que brindaron mejores resultados fueron:\n",
        "- LSTM, en su tercera definicion.\n",
        "- SVM, utilizando BOWs combinados con TF-IDF, con C = 3, kernel = \"rbf\" y gamma =2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VhgTgAWh_D7"
      },
      "source": [
        "#### Prueba con LSTM 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OzTMYHtsq5S",
        "outputId": "e780804a-443f-4dc5-ac42-fa4a136dd20d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Las palabras mas comunes del corpus son: ['no', 'pero', 'si', 'mas', 'hoy', 'gracias', 'muy', 'dia', 'ana', 'ue']\n",
            "Nuestra colección tiene 20030 vectores cargados\n"
          ]
        }
      ],
      "source": [
        "# Evaluación sobre test\n",
        "\n",
        "test_set_2  = test_set.copy()\n",
        "Test_procesado = procesar_corpus(test_set_2)\n",
        "X_testP = [''.join(words[1]) for words in Test_procesado]    #Tweets de Test preprocesados\n",
        "y_test = [label[2] for label in test_set_2]                  #Etiquetas de Test\n",
        "\n",
        "coleccion_vectores_we_lexicos = cargar_top_embeddings(wordvectors,0,Train_procesado,True)\n",
        "embedding_model2 = coleccion_vectores_we_lexicos.copy()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_trainP)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X_test_sequences_lexicos = tokenizer.texts_to_sequences(X_testP)\n",
        "X_test_padded_lexicos = pad_sequences(X_test_sequences_lexicos, maxlen=max_length)\n",
        "\n",
        "\n",
        "embedding_matrix_test = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():                 # Rellenamos la matriz de embedding con los vectores pre-entrenados\n",
        "    if word in embedding_model2:\n",
        "        embedding_matrix_test[i] = embedding_model2[word]\n",
        "\n",
        "y_testHot = etiquetas_one_hot(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxjVCLctCKfO",
        "outputId": "b6278904-1d8c-4f1f-fd2e-332eca6e93c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 21, 302)           6575144   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128)              187904    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,763,435\n",
            "Trainable params: 188,291\n",
            "Non-trainable params: 6,575,144\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "575/575 [==============================] - 212s 365ms/step - loss: 0.6167 - f1_score: 0.7313\n",
            "Epoch 2/10\n",
            "575/575 [==============================] - 194s 338ms/step - loss: 0.4580 - f1_score: 0.7973\n",
            "Epoch 3/10\n",
            "575/575 [==============================] - 188s 327ms/step - loss: 0.4290 - f1_score: 0.8116\n",
            "Epoch 4/10\n",
            "575/575 [==============================] - 215s 375ms/step - loss: 0.4101 - f1_score: 0.8226\n",
            "Epoch 5/10\n",
            "575/575 [==============================] - 244s 425ms/step - loss: 0.3932 - f1_score: 0.8321\n",
            "Epoch 6/10\n",
            "575/575 [==============================] - 182s 317ms/step - loss: 0.3813 - f1_score: 0.8375\n",
            "Epoch 7/10\n",
            "575/575 [==============================] - 246s 428ms/step - loss: 0.3665 - f1_score: 0.8449\n",
            "Epoch 8/10\n",
            "575/575 [==============================] - 227s 395ms/step - loss: 0.3511 - f1_score: 0.8521\n",
            "Epoch 9/10\n",
            "575/575 [==============================] - 203s 353ms/step - loss: 0.3375 - f1_score: 0.8624\n",
            "Epoch 10/10\n",
            "575/575 [==============================] - 186s 323ms/step - loss: 0.3215 - f1_score: 0.8676\n",
            "59/59 [==============================] - 8s 132ms/step\n",
            "\n",
            "Macro LSTM3 para test: 0.5701078\n"
          ]
        }
      ],
      "source": [
        "macro_lstm3 = lstm3(21,X_train_padded_lexicos,y_trainHot,X_test_padded_lexicos,y_testHot,embedding_matrix_test)\n",
        "print('\\nMacro LSTM3 para test: ' + str(macro_lstm3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeTrkKwxh4kD"
      },
      "source": [
        "### Prueba con SVM - BoW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzxEPuuwh3EU"
      },
      "source": [
        "El corpus Test ya fue pre procesado y llevado a los formatos adecuadas en la Parte 2 cuando se realizo el mismo procedimiento para los corpus de Entrenamiento y Desarrollo. Por lo tanto, simplemente hacemos uso de dichas variables globales:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EjxnQeC8sum",
        "outputId": "ec47f3df-b400-41f7-cffb-15937057f80d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 (P):    0.6019417475728156\n",
            "F1 (N):    0.46746575342465757\n",
            "F1 (NONE): 0.6451612903225805\n",
            "Macro-F1: 0.5715229304400179\n"
          ]
        }
      ],
      "source": [
        "model_svm_Best = svm.SVC(C=3, kernel= \"rbf\",gamma= 2)\n",
        "model_svm_Best.fit(X_train_bowP, y_train_mod)\n",
        "y_pred_test_BOW_SVM_Best= model_svm_Best.predict(X_test_bowP)\n",
        "f1_test_BOW_SVM_Best = f1_score(y_test, y_pred_test_BOW_SVM_Best,average=None)\n",
        "imprimir_resultados(f1_test_BOW_SVM_Best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seoglAuDC4WX"
      },
      "source": [
        "Vemos en las salidas anteriores que las Macro-F1 obtenidas son menores a las que se obtuvieron al evaluar el Corpus de Desarrollo. Esto se puede deber a que el tamaño del Corpus de Test es mayor respecto al de Desarrollo, en aproximadamente 700 tweets. Debido a esto se tienen mas tweets en donde fallar la predicción. Ahora veremos como dichos valores son significativamente menores a los valores de Macro-F1 que se pueden obtener utilizando métodos mas nuevos y eficientes como son las redes neuronales basadas en la arquitectura BERT, entre otras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPL5CqLw_fB3"
      },
      "source": [
        "### Evaluamos corpus Test con el modelo pre-entrenado de la biblioteca pysentimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-3zJCwi-gY_"
      },
      "outputs": [],
      "source": [
        "!pip install pysentimiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A55V7gTh7ghy"
      },
      "source": [
        "Para que funcione correctamente la biblioteca, se requiere la siguiente versión de transformers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF5gPAnv7o7Z"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.28.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVv9M6u17za7"
      },
      "source": [
        "Imports necesarios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDLNMhfE7ron"
      },
      "outputs": [],
      "source": [
        "from pysentimiento import create_analyzer\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "a937a1f89b2c440f86140635f2e1952a",
            "02ff6b9119c446e1ac4886ecc8c3ff43",
            "9b591f42289348428c0b464c3d7d78cf",
            "63c7878bbeac47579624398f13aa1648",
            "e75ea20b21dd413e940f474493102753",
            "9fda76b3080f4b91a012664b674cde79",
            "75fd659e48f043e3993f7b7e10496537",
            "f6d208cb0cdd462ea1f048b87e5c8b96",
            "a1748ded3c564cbfbea7b2cc0ba4c556",
            "c819c93ba5b74e599d32b50d71c627b2",
            "f1da2bed9775454cbca6eb4bbb6fec89",
            "ef8240c370e64c6daf23ae3c4f340671",
            "995c4c483b264d5eb9e908c8f5131174",
            "f550683d6ebe47268585cb88445df2c9",
            "b63241c4d9e5435fb2326465c1276f4e",
            "8bb5045e0aac42ca9b81daf81b7a024b",
            "62926e0b9ff44cc5a9bc76ae638bd5b4",
            "1019423cdb444fb7901ccc3f55cf6ae6",
            "ede0e0d736f348d3bc0c8cf9c4ea7826",
            "9196423dcb2d43ed8b873845f9a71291",
            "b36f5bd4466b4847824e935e021b672f",
            "0a48bf6a90dc499e947f7a2b66b106b0",
            "52bcc08db66849aa86651601df983aee",
            "f6a59d90a6c445529b6f0269e7243398",
            "5b35144b8e2e4a4d8189ca39da9fcdb2",
            "5d896fbc87a244c4aaf2b46f187cb55b",
            "a51cb3b001064f19a0c21ddb15f73edb",
            "73a32eb6873a4e7a9bd934a7c1481dd9",
            "6bf52c28785b46a5b1450ee7480d51aa",
            "eaa6c5e61444404f8385d6e559462ed9",
            "c48d05ca768b46899aa407f362a107ea",
            "11835e86101d4e9499fc272202de4266",
            "03cd449bcf354885bf40993254b8c5a9",
            "428c81d745244cfe8cf46c193ff26861",
            "05fd98cce3fb4382a52194bc8e4b2999",
            "9fc8e9827d0543fbb4243c98a70ff722",
            "59669ccddbb94a7983ca930c3f560087",
            "a68f7ee837c943f3b99db2383ea38b9e",
            "0c2116218f0646de8fad077ced1b4ea2",
            "7f7b87734ac643278437edaed32dd205",
            "86b198da5cf340f297a37a643f3435c6",
            "23c7e7bce7974c28abb700bcb8c3ac06",
            "2603fe673d1445839c9fd444a580fe18",
            "88ec7b2e8dc74399b7e908b48f038263",
            "f7a45a3c3de349ce8a48c2e8528f2e81",
            "65447f68a7814771bd583275aa3c952f",
            "ddadfe2620544ee2b7fad3d9e4079792",
            "f428b58f2e8941ee94070dbd0c4f5540",
            "b06bad84d31a4ee1a6a47ccbfb0e8056",
            "cf8d8504f7f74539ae04b9a4f41d0f43",
            "8d8b1e224436426d9c2f2caa695e2336",
            "5d04b242e92a4eb0b533580e96d067db",
            "4b2f8cb80a9141798ad48fadf1275bba",
            "1189ae39f0a546e9a521222717aaef56",
            "69d92ab1eb9345899bc066c4378b0b69"
          ]
        },
        "id": "FBsZlLad77TZ",
        "outputId": "4573cb01-5595-4af6-8b7c-8264d891aa64"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a937a1f89b2c440f86140635f2e1952a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/925 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef8240c370e64c6daf23ae3c4f340671",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/435M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52bcc08db66849aa86651601df983aee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "428c81d745244cfe8cf46c193ff26861",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7a45a3c3de349ce8a48c2e8528f2e81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "transformers.logging.set_verbosity(transformers.logging.ERROR)\n",
        "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
        "\n",
        "def prediccion_pysentimiento(corpus):\n",
        "  res_pysentimiento = []                 #en res_pysentimiento almacenamos las predicciones realizadas por pysentimiento,\n",
        "  for elemento in corpus:                #para todo el corpus de test.\n",
        "    res = analyzer.predict(elemento[1])\n",
        "    res_pysentimiento.append(res)\n",
        "  return res_pysentimiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K30LZXbV8Ca1"
      },
      "outputs": [],
      "source": [
        "def codificar_etiquetas_pysentimiento(y):\n",
        "  etiquetas = {'POS': 0, 'NEG': 1, 'NEU': 2}\n",
        "  return np.array([etiquetas[i] for i in y])\n",
        "\n",
        "\n",
        "def predicciones_a_numerico(y):\n",
        "  ret = []\n",
        "  for elem in y:\n",
        "    ret.append(elem.output)\n",
        "  return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyiQCasZ9DDJ"
      },
      "outputs": [],
      "source": [
        "y_test_numerico = codificar_etiquetas(y_test)          #Implementada en Parte 3\n",
        "\n",
        "prediccion_pys = prediccion_pysentimiento(test_set)\n",
        "y_pysentimiento = predicciones_a_numerico(prediccion_pys)\n",
        "y_pysentimiento_output = codificar_etiquetas_pysentimiento(y_pysentimiento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTG-DUAr9JhM",
        "outputId": "efeeeb8b-c4b6-49d5-fcd6-bc73dba3d0ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro-F1: 0.7041227233971533\n"
          ]
        }
      ],
      "source": [
        "f1_test2 = f1_score(y_test_numerico, y_pysentimiento_output, average=\"macro\")\n",
        "print(\"Macro-F1: \" + str(f1_test2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjjBiOco_6Zf"
      },
      "source": [
        "Vemos que la Macro-F1 coincide prácticamente con el valor declarado en el siguiente sitio web:\n",
        "https://huggingface.co/pysentimiento/robertuito-sentiment-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn-TGtvvst3T"
      },
      "source": [
        "## Preguntas finales\n",
        "\n",
        "Responda las siguientes preguntas:\n",
        "\n",
        "1) ¿Qué modelos probaron para la representación de los tweets?\n",
        "\n",
        "\\\n",
        "\n",
        "Para la representación de tweets probamos los modelos basados en Bag of Words (BOW) y con Word Embeddings (WE). En el caso de BOW, utilizamos el enfoque que combina BOW con Tf-Idf, y el enfoque que selecciona las k-features mas releavantes del corpus de entrenamiento. La combinación con Tf-Idf resulta útil debido al hecho de que esta tiene en cuenta tanto la frecuencia de aparición de una palabra en un tweet como su rareza en el corpus. Esto significa que las palabras menos frecuentes pero más distintivas tienen un mayor peso, lo que ayuda a capturar mejor la semántica y el contexto del tweet para el análisis sentimental. A su vez dicha aplicación de la técnica Idf, las palabras que aparecen en muchos tweets reciben un puntaje más bajo, lo que les resta importancia y ayuda a filtrar el ruido y las palabras menos informativas. Una desventaja de este enfoque es el tamaño de los vectores que se necesitan para representar los tweets. Estos, por las características de los datos, poseen una dimension mayor a 21000.\n",
        "\n",
        "Por otro lado el enfoque que selecciona las k-features mas relevantes nos parecía una buena opción a analizar, ya que permite reducir el tamaño de la representación de los tweets y a su vez, como se observó y se volvera a comentar mas adelante, permitió para ciertos valores de hiperparámetros obtener mejores resultados de Macro-F1 respecto al enfoque anterior. Los valores de k que consideramos fueron 250, 500 y 1000. Para obtener la representación, primero seleccionabamos a partir de un Data Set de entrenamiento, las k-features mas relevantes. Una vez obtenido dicho vocabulario se obtenia la representacion de los tweets a partir de la funcion de puntuación $chi$, que evaluaba la secuencia de palabras respecto a dicho vocabulario obtenido anteriormente, generando el BOW que representara al tweet. En relación a lo anterior, se experimento tambien agregar dos valores al final de dichos vectores de dimension $k$. Para dicho caso, el valor de la anteúltima coordenada representaria la cantidad de palabras del tweet que estan incluidas en el lexico positivo brindado via EVA. De forma similar, la última coordenda tendria la cantidad de palabras del tweet que estan incluidas en el léxico negativo.\n",
        "\n",
        "Respecto a las otros posibles enfoques que se daban como opciones, BOW estandar requería vectores del mismo tamaño que los generados por el enfoque Tf-Idf aunque no consideraba importancia de las palabras por lo que vimos mas adecuado el que utiliza Tf-Idf. BOW filtrando stop-words no corresponde debido a que se filtran las stop-words al preprocesar los corpus por lo que ya estaria siendo considerado. Por ultimo, BOW usando lemas requería tiempos enormes de ejecucion para obtener las representaciones por lo que optamos por prescindir del mismo.\n",
        "\n",
        "En cuanto a Word Embeddings, probamos ambas representaciones sugeridas en la letra, es decir, tomamos tanto el promedio de los vectores de word embeddings de las palabras de cada tweet para representarlo como la concatenación de los word embeddings. Observamos que la primera representación otorga mejores tiempos de procesamiento que representarlos como la concatenación, esto debido a que tomamos un vector de tamaño mucho menor, pero al no tener la concatenación de los vectores, podemos estar perdiendo información semántica al tomar el promedio, por lo que consideramos que la segunda representación es mejor en este aspecto.\n",
        "\n",
        "Utilizamos un conjunto de vectores predefinidos para representar nuestros word embeddings, pero observamos que esto podía ser ineficiente al procesar los corpus, por lo que tomamos de nuestro conjunto \"train\" de tweets todas las palabras que se obtienen luego de aplicar el preprocesamiento, obteniendo así\n",
        "una colección de vectores mucho mas pequeña otpimizando los tiempos de ejecución y representando los tweets casi en su totalidad como lo haríamos con los word embeddings importados.\n",
        "\n",
        "Finalmente, consideramos añadir una representación de los léxicos que fueron proporcionados por la letra en la representacion de cada tweet, añadiendo dos valores al final de cada vector, cada uno de estos valores representará la cantidad de palabras positivas y negativas respectivamente que tiene el tweet correspondiente, como ya fue mencionado.\n",
        "\n",
        "\\\n",
        "2) ¿Aplicaron algún tipo de preprocesamiento de los textos?\n",
        "\n",
        "\\\\\n",
        "Se decidió aplicar preprocesamiento tanto al corpus de entrenamiento como al corpus de test, antes de su evaluación. Esto con el fin de eliminar stop words, que consideramos fundamental a la hora de trabajar con BOW, para reducir el tamaño de los vectores generados. A su vez estas no aportan valor al análisis de sentimiento.\n",
        "Por otro lado, se eliminaron menciones, urls, tíldes,secuencias de tres o mas veces la misma letra repetida, se sustituyeron groserias por la palabra \"insulto\", perteneciente al conjunto de lemas \"negativos\". De forma análoga, las risas se sustituyeron por \"jajaja\", presente en el conjunto de lemas positivos, para que las risas sean uniformes a su vez. Ademas, se eliminaron los numeros y el simbolo \"#\", dejando el contendio a continuación del mismo.\n",
        "\n",
        "\\\\\n",
        "3) ¿Qué modelos de aprendizaje automático probaron?\n",
        "\n",
        "\\\\\n",
        "\n",
        "Decidimos probar los modelos de aprendizaje automático basado en atributos mencionados en la letra, es decir, Multi Layer Perceptron (MLP) y Support Vector Machines (SVM). Probamos ambos modelos con diferentes representaciones. Para el caso de word embeddings utilizamos la representación en promedio de los vectores de las palabras de cada tweet preprocesado. Para Bag of Words, elegimos los dos últimos enfoques sugeridos como se mencionó anteriormente: seleccionar las features más relevantes con SelectKBest y BOW combinado con TF-IDF utilizando TfidfVectorizer.\n",
        "\n",
        "\\\\\n",
        "\n",
        "A su vez probamos los modelos Naive Bayes y Regresión Logística declarando un clasificador simple para cada uno de ellos, a modo de experimentación. Para  Naive Bayes utilizamos las entradas representadas mediante BOWs con enfoque Tf-Idf, mientras que para Regresión Logística utilizamos tanto esta representación como también mediante word embeddings promedio. Los resultados sugirieron que Naive Bayes no parece ser una buena opción para esta tarea de PLN, probablemte debido a la independencia condicional que este asume. Por otro lado, el modelo Regresión Logística mostró muy buenos resultados, obteniendose una Macro-F1 de 0.57 aproximadamente para BoW y 60.0 para Word Embeddings Promedio.\n",
        "\n",
        "\\\\\n",
        "4) ¿Qué atributos utilizaron para estos modelos?\n",
        "\n",
        "\\\\\n",
        "En el caso de BOW, se utilizaron los enfoques mencionados anteriormente: select k-features y Tf-Idf, para utilizar a las palabras de los tweets del corpus como atributos, representadas por *X_train_bowP*, donde eran almacenados los BOW de cada tweet previamente preprocesado, del corpus de entrenamiento.\n",
        "\n",
        "\n",
        "En cuanto a otras variables que se utilizaron para entrenar el modelo y realizar la clasificación, teniamos los atributos categóricos almacenados en vectores. El nombre de dichos vectores comenzaba siempre con la letra \"y\", y fueron transformados a distintos formatos, segun la función de pérdida utilizada en los clasificadores. Entre ellas, One-hot encoding.\n",
        "\n",
        "\n",
        "Por otro lado, en BOW combinado con select k-features, podriamos considerar que el valor de K era un atributo que permitía definir la variable de entrada, y una vez iniciado el entrenamiento este no se ajustaba. También lo consideramos hiperparámetro porque lo seleccionamos en algunos casos segun una función de busqueda, aunque no nos quedamos solo con ese valor sino que probamos con tres.\n",
        "\n",
        "Tanto para el modelo SVM como para MLP, se utilizaron WE pre-entrenados para representar a traves tanto del vector promedio como del vector concatenación a los tweets como atributos. Decidimos generar atributos incorporando los léxicos a nuestra representación contando las palabras positivas y negativas de los tweets e inluyendolos como un número en el vector. Finalmente, para la parte de LSTM, implementamos una capa de Embedding utilizando tanto los vectores de las palabras de forma independiente. Para LSTM 1 y 2, utilizamos vectores de largo 300 sin agregar información de los léxicos, mientras que para LSTM 3 añadimos información de los léxicos obteniendo vectores de largo 302 cuyas dos ultimas coordenadas indican si la palabra correspondiente se encuentra o no en alguno de los dos léxicos.\n",
        "\n",
        "\n",
        "\\\\\n",
        "5) ¿Probaron algún enfoque de aprendizaje profundo?\n",
        "\n",
        "\\\\\n",
        "\n",
        "Probamos el modelo LSTM de redes neuronales recurrentes, donde definimos tres redes LSTM distintas.\n",
        "\n",
        "\n",
        "Antes de definir las redes, obtuvimos una matriz con los Word Embeddings de las palabras que aparecen en el Corpus de entrenamiento. De esta forma aprovechamos WE pre-entrenados en una capa Embedding de las redes LSTM que describiremos a continuación. Tambien llevamos a los conjuntos de datos de entrenamiento y desarrollo al formato adecuado. Dicho formato refiere a representar mediante índices al conjunto de tweets del corpus de entrenamiento modificado y preprocesado. Luego, con dicho resultado obtenido, llevamos todas las secuencias de índices al mismo largo mediante el uso de la función pad_sequences. A dicha función le indicabamos el largo máximo que un tweet ya sea del corpus de entrenamiento o del de desarrollo, podia tener. En caso de secuencias menores a dicho largo, se le agregaban ceros al incio del vector. Dichas secuencias de vectores de mismo largo son las que finalmente reciben como entradas los modelos de LSTM. Luego las secuencias de etiquetas esperadas de los conjuntos fueron codificadas en formato One-Hot Encoding.\n",
        "\n",
        "La primer red LSTM utilizaba los WE pre-entrenados que fueron cargados en Parte 2 y puestos en una matriz. La primer Red, disponia para entrenar mas de 85.000 parámetros, contando con otros 6.600.000 de parámetros no entrenables aproximadamente, pues quedaron determinados por los WE pre-entrenados.  Utilizamos tambien una Capa Bidireccional LSTM para capturar tanto el contexto pasado como el futuro de las secuencias en dicha red. A su vez hicimos que el modelo utilizara el optimizador Adam, la función de pérdida categorical_crossentropy y se utiliza la métrica F1 Score para evaluar el rendimiento. La misma fue entrenada durante 5 epocas.\n",
        "\n",
        "\n",
        "En cuanto a la segunda red considerada, volvimos a entrenar durante 5 épocas. Respecto al modelo anterior, pasamos a utilizar capas mas variadas en busca de mejorar la performance. Entre dichas capas, optamos por dos capas densas (Dense) con activación ReLU para aprender representaciones más abstractas de los datos, una capa de convolución unidimensional y capas de SpatialDropout y Dropout que son incluidas para regularizar los datos y reducir el sobreajuste.\n",
        "Ademas definimos un callback ReduceLROnPlateau2 para reducir la tasa de aprendizaje cuando la pérdida en el conjunto de validación dejaba de mejorar.\n",
        "\\\n",
        "Para realizar pruebas respecto a representar los WE incluyendo información de los léxicos como fué mencionado en la Respuesta 5, decidimos probar con ambos modelos, es decir, tanto con la red LSTM 1 como con la red LSTM 2, pero observamos que la red LSTM 1 aportaba mejores resultados, por lo que decidimos solo incluir esta red en la notebook, llamándola LSTM 3. Por lo tanto, como representamos la información que nos aportan los léxicos como dos coordenadas más para nuestros vectores de palabras la red LSTM 3 tiene como entrada de su capa de embedding 302 de dimensión de entrada. Observamos que la Macro-F1 era creciente conforme más epocas utilizábamos, por lo que decidimos fijar un número alto de épocas con respecto a las demás (10).\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "6) ¿Probaron diferentes configuraciones de hiperparámetros?\n",
        "\n",
        "\n",
        "\n",
        "Los hiperparámetros de los clasificadores son parámetros que no se aprenden directamente del conjunto de datos, pero que afectan el proceso de entrenamiento y la forma en que se construyen los modelos. Con el fin de obtener el mejor rendimiento posible de los clasificadores, utilizamos diversos hiperparametros para dicho fin, ajustandolos segun los valores de Macro-F1 que eran obtenidos.\n",
        "\n",
        "En el caso del modelo SVM, utilizamos los siguientes hiperparámetros:\n",
        "\n",
        "1. C (parámetro de regularización): Controla el equilibrio entre lograr un margen más amplio y minimizar el error de clasificación en el conjunto de entrenamiento. Valores altos de C penalizarán más los errores de clasificación, aunque hacen al modelo más complejo.\n",
        "\n",
        "2. kernel (núcleo): Especifica la función de transformación utilizada para mapear los datos de entrada en un espacio de mayor dimensión, donde se puede realizar una separación lineal. Los kernels que consideramos son:\n",
        "\n",
        "   - Lineal (linear): No aplica ninguna transformación y utiliza una función lineal para la separación.\n",
        "   - Sigmoide (sigmoid): Utiliza una función sigmoide para realizar la transformación.\n",
        "   - Radial Basis Function (rbf): Transforma el espacio de características original en un espacio de características de mayor dimensión, lo que permite una separación no lineal de las clases. Esta mapea los datos a través de una función no lineal que hace que las muestras sean más fácilmente separables por un hiperplano en el espacio transformado.\n",
        "   - Polinomial (poly): se mapean los datos de entrada a un espacio de características polinomial en el que utilizamos el paramétro \"degree\" para determinar el grado del polinomio.\n",
        "\n",
        "  Dichos tipos de kernel son los mas comunes, y en particular el kernel mas utilizado en clasificadores SVM para anlisis de sentimiento es \"rbf\"\n",
        "\n",
        "3. gamma : Es un hiperparámetro crítico en el kernel RBF, un hiperparámetro de ajuste que controla la influencia de cada muestra en el modelo.\n",
        "\n",
        "\n",
        "Como los valores óptimos de dichos hiperparámetros varian según el conjunto de datos, decidimos utilizar técnicas de validación cruzada para encontrar los valores óptimos que brinden un mejor rendimiento. Para ello utilizamos GridSerach que nos permitia probar 6 combinaciones diferentes, involucrando estas a tres valores diferentes de \"C\" y las dos posibles funciones para kernel \"linear\" y \"sigmoid\". El siguiente código aplicaba la mencionada técnica de validación cruzada con k =5:\n",
        "\n",
        "\\\\\n",
        "\n",
        "model_svm = svm.SVC(random_state=1234)\n",
        "\n",
        "\n",
        "param_dict = {'C': [ 5, 10, 20],\n",
        "             'kernel': ['linear', 'sigmoid']}\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(model_svm, param_dict, scoring='f1_macro')\n",
        "\n",
        "\n",
        "grid_search.fit(X_train_bowP, y_train)   #X_train_bowP es el data_set de entrenamiento representado con BOW combiando con TF-IDF.\n",
        "\n",
        "\n",
        "\\\\\n",
        "Finalmente la grilla arrojo que los valores que arrojaron mejor valor de Macro-F1 fueron C=5 y kernel = \"linear\".\n",
        "\n",
        "\n",
        "Sin embargo, como se explicó en la parte 3, optamos por probar mas valores ya que el resultado obtenido de la validación cruzada no era muy convincente. De este modo obtuvimos mejores valores con kernel = \"rbf\", para los tres valores de C: 3, 8 y 20. Eso nos llevo a probar con diferentes valores de gamma, pues este actua en conjunto con kernel rbf, a lo que obtuvimos mejores resultados para C = 3 y gamma 0.1 o 2.\n",
        "\n",
        "\\\\\n",
        "\n",
        "En cuanto a MLP utilizamos los hiperparámetros:\n",
        "\n",
        "1. Número de capas ocultas: El MLP consta de una o más capas ocultas entre la capa de entrada y la capa de salida.\n",
        "\n",
        "\n",
        "2. Función de activación: La función de activación se aplica a cada neurona en el MLP y permite la introducción de no linealidad en el modelo. Las funciones de activación consideradas fueron la función sigmoide (sigmoid),y la función de activación rectificada lineal (ReLU).\n",
        "\n",
        "3. Alpha: El hiperparámetro alpha controla la fuerza de la regularización L2. Es decir, actúa sobre los pesos grandes de manera que valores grandes de alpha penalizan más los pesos grandes y reducen la complejidad del modelo, lo que puede ayudar a prevenir el sobreajuste. Por otro lado, un valor más bajo de alpha permite que los pesos tomen valores más grandes y, por lo tanto, puede permitir un modelo más complejo y flexible, pero con mayor riesgo de sobreajuste.\n",
        "\n",
        "Al igual que con SVM, utilizamos validación cruzada para determinar que valores optimizaban el valor de la Macro-F1 para el conjunto de entrenamiento representado con BOW combiandos con TF-IDF. Los posibles valores para los hiperparámetros mencionados anteriormente eran:\n",
        "\n",
        "\\\\\n",
        "param_dict = {'hidden_layer_sizes': [(1,), (3,), (10,)],\n",
        "              'activation': ['relu', 'logistic'],\n",
        "              'alpha': [0.0001, 0.001, 0.01]}\n",
        "\n",
        "\\\\\n",
        "Luego de la prueba, obtuvimos que los mejores valores eran: hidden_layer_sizes = (3,), activation = \"relu\", alpha = 0.0001, aunque al igual que con SVM probamos otros valores. Sin embargo, los valores sugeridos para los hiperparámetros: hidden_layer_sizes = (3,), activation = \"logistic\" y alpha = 0.01, coincidieron con los que daban una mejor Macro-F1 para tweets representados con los distintos enfoques de BOW.\n",
        "\n",
        "Para Word embeddings concatenación no se pudo llevar a cabo esta medida para ninguno de los dos modelos por el tiempo que supone ejecutar, esto se debe a que los word embeddings concatenados que tomamos son vectores de tamaño 3002 al tomar 10 palabras, por lo que tanto para los concatenados probamos distintos valores manualmente. Para Word Embeddings promedio utilizamos Gridsearch en MLP, obteniendo buenos resultados, pero en SVM decidimos además probar valores manualmente.\n",
        "\\\\\n",
        "\n",
        "Finalmete para LSTM utilizamos los hiperparametros:\n",
        "\n",
        "1. Embedding (\"input_dim\", \"output_dim\", \"input_length\")\n",
        "\n",
        "2. Bidirectional LSTM\n",
        "\n",
        "3. Dense\n",
        "\n",
        "4. Activation (\"softmax\")\n",
        "\n",
        "5. Optimizador (\"adam\")\n",
        "\n",
        "6. Loss (funcion de perdida: 'categorical_crossentropy')\n",
        "\n",
        "7. Epochs\n",
        "\n",
        "La explicación de los mismos fue resumida en la parte anterior.\n",
        "\n",
        "En cuanto a los valores de dichos hiperparámetros, realizamos diferentes ejecuciones, donde ibamos variando los mismos. Dichas variaciones determinaban el número de parametros a entrenar de la red, para el cual no nos podíamos exceder debido a que la ejecución podia llegar a no ser soportada por la limitación de la RAM del entorno de ejecución. Finalmente, nos quedamos con la configuración de valores que arrojó mejores resultados, para cada red.\n",
        "\n",
        "\\\\\n",
        "\n",
        "7) ¿Qué enfoque (preprocesamiento + representación de tweets + modelo + atributos/parámetros) obtuvo la mejor Macro-F1?\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "En la siguiente tabla, se indica los mejores resultados obtenidos, para cada representación bajo la medida Macro-F1. Las entradas vacías simbolizan que no se utilizo el clasificador con esa representación, ya sea, por la naturaleza del clasificador, como por ejemplo naive bayes que se basa en recuentos de palabras o la frecuencia de términos siendo mas conveniente la representación BOW; o simplemente por simplicidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "f8ClPbDiGR3E",
        "outputId": "d4638c20-8c17-40d2-a243-eda899261e1b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<table style=\"border-collapse: collapse;\">\n",
              "  <tr style=\"background-color: gray; color: white;\">\n",
              "    <th></th>\n",
              "    <th style=\"padding: 8px;\">BOW+TF-IDF</th>\n",
              "    <th style=\"padding: 8px;\">BOW+k-feat.</th>\n",
              "    <th style=\"padding: 8px;\">BOW+k-f.+2 coord.</th>\n",
              "    <th style=\"padding: 8px;\">WE(mean vector)</th>\n",
              "    <th style=\"padding: 8px;\">WE(concat)</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"padding: 8px;\">MLP</td>\n",
              "    <td style=\"padding: 8px;\">0.556</td>\n",
              "    <td style=\"padding: 8px;\">0.562</td>\n",
              "    <td style=\"padding: 8px;\">0.562</td>\n",
              "    <td style=\"padding: 8px;\">0.581</td>\n",
              "    <td style=\"padding: 8px;\">0.561</td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"padding: 8px;\">SVM</td>\n",
              "    <td style=\"padding: 8px;\">0.598</td>\n",
              "    <td style=\"padding: 8px;\">0.533</td>\n",
              "    <td style=\"padding: 8px;\">0.549</td>\n",
              "    <td style=\"padding: 8px;\">0.597</td>\n",
              "    <td style=\"padding: 8px;\">0.522</td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"padding: 8px;\">NAIVE BAYES</td>\n",
              "    <td style=\"padding: 8px;\">0.417</td>\n",
              "    <td style=\"padding: 8px;\">-</td>\n",
              "    <td style=\"padding: 8px;\">-</td>\n",
              "    <td style=\"padding: 8px;\">-</td>\n",
              "    <td style=\"padding: 8px;\">-</td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"padding: 8px;\">REGRESIÓN LOG.</td>\n",
              "    <td style=\"padding: 8px;\">0.575</td>\n",
              "    <td style=\"padding: 8px;\">-</td>\n",
              "    <td style=\"padding: 8px;\">-</td>\n",
              "    <td style=\"padding: 8px;\">0.602</td>\n",
              "    <td style=\"padding: 8px;\">-</td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"padding: 8px;\">LSTM</td>\n",
              "    <td style=\"padding: 8px;\">-</td>\n",
              "    <td style=\"padding: 8px;\">-</td>\n",
              "    <td style=\"padding: 8px;\">-</td>\n",
              "    <td style=\"padding: 8px;\">0.600</td>\n",
              "    <td style=\"padding: 8px;\">-</td>\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "tabla_html = \"\"\"\n",
        "<table style=\"border-collapse: collapse;\">\n",
        "  <tr style=\"background-color: gray; color: white;\">\n",
        "    <th></th>\n",
        "    <th style=\"padding: 8px;\">BOW+TF-IDF</th>\n",
        "    <th style=\"padding: 8px;\">BOW+k-feat.</th>\n",
        "    <th style=\"padding: 8px;\">BOW+k-f.+2 coord.</th>\n",
        "    <th style=\"padding: 8px;\">WE(mean vector)</th>\n",
        "    <th style=\"padding: 8px;\">WE(concat)</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding: 8px;\">MLP</td>\n",
        "    <td style=\"padding: 8px;\">0.556</td>\n",
        "    <td style=\"padding: 8px;\">0.562</td>\n",
        "    <td style=\"padding: 8px;\">0.562</td>\n",
        "    <td style=\"padding: 8px;\">0.581</td>\n",
        "    <td style=\"padding: 8px;\">0.561</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding: 8px;\">SVM</td>\n",
        "    <td style=\"padding: 8px;\">0.598</td>\n",
        "    <td style=\"padding: 8px;\">0.533</td>\n",
        "    <td style=\"padding: 8px;\">0.549</td>\n",
        "    <td style=\"padding: 8px;\">0.597</td>\n",
        "    <td style=\"padding: 8px;\">0.522</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding: 8px;\">NAIVE BAYES</td>\n",
        "    <td style=\"padding: 8px;\">0.417</td>\n",
        "    <td style=\"padding: 8px;\">-</td>\n",
        "    <td style=\"padding: 8px;\">-</td>\n",
        "    <td style=\"padding: 8px;\">-</td>\n",
        "    <td style=\"padding: 8px;\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding: 8px;\">REGRESIÓN LOG.</td>\n",
        "    <td style=\"padding: 8px;\">0.575</td>\n",
        "    <td style=\"padding: 8px;\">-</td>\n",
        "    <td style=\"padding: 8px;\">-</td>\n",
        "    <td style=\"padding: 8px;\">0.602</td>\n",
        "    <td style=\"padding: 8px;\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding: 8px;\">LSTM</td>\n",
        "    <td style=\"padding: 8px;\">-</td>\n",
        "    <td style=\"padding: 8px;\">-</td>\n",
        "    <td style=\"padding: 8px;\">-</td>\n",
        "    <td style=\"padding: 8px;\">0.600</td>\n",
        "    <td style=\"padding: 8px;\">-</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(tabla_html))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvuUXHA9HlgK"
      },
      "source": [
        "El primer enfoque que dió la mejor Macro-F1 fué la Red Neuronal LSTM3, que consiste en aplicar el preprocesamiento habitual para el corpus de Train Procesado, el cual además de agregar los valores de los léxicos como tweets, agrega tweets inventados como 'NONE' en orden, es decir, las categorías del corpus están intercaladas como 'NONE' 'P' 'N' 'NONE' ... obteniendo un corpus equilibrado que observamos que da buenos resultados para las redes LSTM. Respecto a la representación, utilizamos los WE con atributos generados por los léxicos como fué mencionado anteriormente. Respecto a los parámetros, como ya se mencionó, debimos de utilizar 302 de dimensión de entrada en la capa de embedding, los hiperparámetros de mejor resultado para este modelo fueron: 10 épocas de entrenamiento, tamaño de batch de 32, optimizador adam y 64 unidades en la capa LSTM.\n",
        "\n",
        "El segundo enfoque que obtuvo la mejor Macro-F1 consistía en aplicar el preprocesamiento habitual para los Corpus de entrenamiento y desarrollo. Luego representar los tweets mediante BOWs combinado con Tf-Idf, y codificando el conjunto de etiquetas mediante One-Hot Encoding, para pasarle finalmente dichas variables a un modelo SVM con hiperparámetros de valor C=3, kernel=rbf y gamma=2. Dado todo lo anterior, se obtuvo una Macro-F1 igual a 0.5976970560303894\n",
        "\n",
        "\\\\\n",
        "\n",
        "8) ¿Qué clase es la mejor clasificada por este enfoque? ¿Cuál es la peor? ¿Por qué piensan que sucede esto?\n",
        "\n",
        "\\\\\n",
        "Respecto al enfoque anterior de LSTM (3), traemos los datos obtenidos de la correspondiente seccion donde se realizo la prueba y se obtuvo:\n",
        "\n",
        "\\\\\n",
        "\n",
        "F1 (P):    0.6272618\n",
        "F1 (N):    0.66490066\n",
        "F1 (NONE): 0.5088235\n",
        "Macro-F1: 0.6003286838531494\n",
        "\n",
        "En un principio, la F1 de None estaba muy alejada de este valor obtenido. Para intentar mejorarla, se introdujeron como ya fue mencioando 3354 \"tweets\" nuevos para cada clase, mezclados entre ellos. Dicha estrategia mejoró los resultados en todos los modelos en general, donde se aplicó dicho Corpus modificado.\n",
        "\n",
        "Observamos que la clase mejor clasificada son los Negativos, mientras que la peor clase son los Neutros. Consideramos que esto sucede porque en este enfoque aplicamos tanto la estategia de generar tweets a partir de los léxicos como la de añadir atributos de cantidad de palabras negativas y positivas a los word-embeddings. Ambas acciones potencian las dos clases con ventaja en este método.\n",
        "\n",
        "\\\\\n",
        "\n",
        "9) ¿Cómo son sus resultados en comparación con los de pysentimiento? ¿Por qué piensan que sucede esto?\n",
        "\n",
        "\\\\\n",
        "\n",
        "Los resultados son inferiores respecto a los que se logran mediante pysentimiento. Esto se debe a varias razones. Entre ellas:\n",
        "\n",
        "\n",
        "1. Preentrenamiento en datos relevantes: Pysentimiento se entrena en un gran conjunto de datos de texto, que incluye una amplia variedad de expresiones y estilos de lenguaje. Dentro de los Data sets que se utilizaron para entrenarlo, se encuentran data sets proprcionados por TASS, el mismo grupo que creó el corpus de entrenamiento que utilizamos en el laboratorio. Cabe destacar que aunque nuestros corpus no los tenían, pysentimiento se entrenó también con tweets que poseen emojis, entre otras características.\n",
        "\n",
        "2. Pysentimiento está diseñado para capturar y analizar el contexto y la emotividad de los textos, lo que le permite comprender y extraer los sentimientos expresados en los tweets de manera efectiva.\n",
        "\n",
        "3. Uso de técnicas de procesamiento de lenguaje natural avanzadas: Pysentimiento utiliza técnicas de procesamiento de lenguaje natural avanzadas, como el uso de modelos de lenguaje preentrenados y algoritmos de aprendizaje automático, para comprender y analizar los textos de manera más precisa. Estas técnicas permiten que pysentimiento capture la información semántica y sintáctica relevante de los tweets, lo que contribuye a una mejor detección de sentimientos. Uno de dichos algoritmos de Deep Learning, está basado en Redes neuronales de arquitectura Transformers. Dichas redes representan hoy en día el estado del arte en varias ramas de PLN, entre ellas Análisis de Sentimiento.\n",
        "\n",
        "4. Adaptación a dominios específicos: Pysentimiento se ha entrenado en una variedad de dominios, lo que le permite adaptarse a diferentes contextos y estilos de texto. Los tweets son un dominio específico con características lingüísticas y emocionales únicas, y el entrenamiento de pysentimiento en un conjunto de datos diverso ayuda a capturar esas características y lograr un rendimiento sólido en el análisis de sentimientos en tweets.\n",
        "\n",
        "\\\\\n",
        "En nuestro caso, nos restringimos a probar distintos algoritmos de aprendizaje automático y de Deep Learning, sin procurar combinar estos ya que el objetivo era experimentar con distintos modelos que se presentaron a lo largo del curso.\n",
        "\n",
        "Por otro lado, nuestro Corpus de entrenamiento es significativamente menor al utilizado por pysentimiento. El utilizado en este laboratorio contaba con solamente 8314 tweets, aunque como ya fue mencionado, utilizamos los conjuntos de léxicos positivos y negativos para llevarlo a 18373 \"tweets\". Dicho número no es comparable a los 500 millones de tweets que contaba el corpus de entrenamiento de pysentimiento. Otro punto relacionado a lo anterior, es que estos se encuentran desbalanceados, lo que provoca que en algunos métodos, una de las tres clases tenga un rendimiento destacablemente inferior al resto. Observamos que para los métodos que utilizan el corpus train sin agregar información adicional, la clase 'N' es la que obtiene una peor F1 de las tres categorías, mientras que para los enfoques que utilizan información de los léxicos, esta clase es 'NONE', lo cual tiene mucho sentido ya que en train normal es donde hay menos tweets negativos y al agregar información de léxicos es natural que aumente la precisión de las clases 'P' y 'N'.\n",
        "\n",
        "Otro factor a tener en cuenta, es que además de hacer uso de modelos mas actuales y potentes, al tratarse de un proyecto de semejante porte, se habrá contado con muchas mas horas de pruebas analíticas y de Ensayo-Error en busca de optimizar modelos que ya de por si seguramente ofrecían resultados mejores a los nuestros.\n",
        "\n",
        "En relación a lo anterior, para formar la matriz de embeddings que pasamos como parámetro a las redes LSTM, solo pudimos considerar los embeddings correspondientes a las palabras que aparecían en el corpus de desarrollo. Esto debido a que estabamos restringidos por la RAM del ambiente Colab. De otro modo con mas recursos se podría, por ejemplo, pasarle a dichas redes los 2.000.000 de WE que brindaba el archivo .vec con los WE pre-entrenados que utilizamos para la tarea, en lugar de los aproximadamente 21000 WE que optamos por pasarle a la capa Embedding de tal modelo. Incluso, algunos de esos WE, eran secuencias de 300 ceros debido a palabras que el preprocesamiento no pudo eliminar y no existe, o que no fueron incluidas en el archivo .vec.\n",
        "\n",
        "Para nuestra sorpresa, si bien BoW es un método más clásico que word embeddings o Deep Learning, obtuvo muy buen resultado para SVM obteniendo el segundo lugar\n",
        "de todos los modelos en cuanto a Macro-F1:\n",
        "\n",
        "F1 (P):    0.6296296296296297 \\\\\n",
        "F1 (N):    0.5 \\\\\n",
        "F1 (NONE): 0.6634615384615384 \\\\\n",
        "\n",
        "Macro-F1: 0.5976970560303894 \\\\"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zd4KI-_N9nmJ",
        "czM9aJo1QNOW",
        "bb1Dp929VBsl",
        "o49MgHouVJLV",
        "Y91AcNtzTpGM",
        "KXvpeECB5te4",
        "wa6uwGRkKuQ5",
        "m8N7inTNKuQ7",
        "1dIngCyjNL6-",
        "111v4cl-LllO"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02ff6b9119c446e1ac4886ecc8c3ff43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fda76b3080f4b91a012664b674cde79",
            "placeholder": "​",
            "style": "IPY_MODEL_75fd659e48f043e3993f7b7e10496537",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "03cd449bcf354885bf40993254b8c5a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05fd98cce3fb4382a52194bc8e4b2999": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c2116218f0646de8fad077ced1b4ea2",
            "placeholder": "​",
            "style": "IPY_MODEL_7f7b87734ac643278437edaed32dd205",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "0a48bf6a90dc499e947f7a2b66b106b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c2116218f0646de8fad077ced1b4ea2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1019423cdb444fb7901ccc3f55cf6ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11835e86101d4e9499fc272202de4266": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1189ae39f0a546e9a521222717aaef56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c7e7bce7974c28abb700bcb8c3ac06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2603fe673d1445839c9fd444a580fe18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "428c81d745244cfe8cf46c193ff26861": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05fd98cce3fb4382a52194bc8e4b2999",
              "IPY_MODEL_9fc8e9827d0543fbb4243c98a70ff722",
              "IPY_MODEL_59669ccddbb94a7983ca930c3f560087"
            ],
            "layout": "IPY_MODEL_a68f7ee837c943f3b99db2383ea38b9e"
          }
        },
        "4b2f8cb80a9141798ad48fadf1275bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52bcc08db66849aa86651601df983aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6a59d90a6c445529b6f0269e7243398",
              "IPY_MODEL_5b35144b8e2e4a4d8189ca39da9fcdb2",
              "IPY_MODEL_5d896fbc87a244c4aaf2b46f187cb55b"
            ],
            "layout": "IPY_MODEL_a51cb3b001064f19a0c21ddb15f73edb"
          }
        },
        "59669ccddbb94a7983ca930c3f560087": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2603fe673d1445839c9fd444a580fe18",
            "placeholder": "​",
            "style": "IPY_MODEL_88ec7b2e8dc74399b7e908b48f038263",
            "value": " 1.31M/1.31M [00:00&lt;00:00, 14.1MB/s]"
          }
        },
        "5b35144b8e2e4a4d8189ca39da9fcdb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaa6c5e61444404f8385d6e559462ed9",
            "max": 384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c48d05ca768b46899aa407f362a107ea",
            "value": 384
          }
        },
        "5d04b242e92a4eb0b533580e96d067db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d896fbc87a244c4aaf2b46f187cb55b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11835e86101d4e9499fc272202de4266",
            "placeholder": "​",
            "style": "IPY_MODEL_03cd449bcf354885bf40993254b8c5a9",
            "value": " 384/384 [00:00&lt;00:00, 6.09kB/s]"
          }
        },
        "62926e0b9ff44cc5a9bc76ae638bd5b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63c7878bbeac47579624398f13aa1648": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c819c93ba5b74e599d32b50d71c627b2",
            "placeholder": "​",
            "style": "IPY_MODEL_f1da2bed9775454cbca6eb4bbb6fec89",
            "value": " 925/925 [00:00&lt;00:00, 40.1kB/s]"
          }
        },
        "65447f68a7814771bd583275aa3c952f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf8d8504f7f74539ae04b9a4f41d0f43",
            "placeholder": "​",
            "style": "IPY_MODEL_8d8b1e224436426d9c2f2caa695e2336",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "69d92ab1eb9345899bc066c4378b0b69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bf52c28785b46a5b1450ee7480d51aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73a32eb6873a4e7a9bd934a7c1481dd9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75fd659e48f043e3993f7b7e10496537": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f7b87734ac643278437edaed32dd205": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86b198da5cf340f297a37a643f3435c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88ec7b2e8dc74399b7e908b48f038263": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bb5045e0aac42ca9b81daf81b7a024b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d8b1e224436426d9c2f2caa695e2336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9196423dcb2d43ed8b873845f9a71291": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "995c4c483b264d5eb9e908c8f5131174": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62926e0b9ff44cc5a9bc76ae638bd5b4",
            "placeholder": "​",
            "style": "IPY_MODEL_1019423cdb444fb7901ccc3f55cf6ae6",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "9b591f42289348428c0b464c3d7d78cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6d208cb0cdd462ea1f048b87e5c8b96",
            "max": 925,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1748ded3c564cbfbea7b2cc0ba4c556",
            "value": 925
          }
        },
        "9fc8e9827d0543fbb4243c98a70ff722": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86b198da5cf340f297a37a643f3435c6",
            "max": 1306803,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23c7e7bce7974c28abb700bcb8c3ac06",
            "value": 1306803
          }
        },
        "9fda76b3080f4b91a012664b674cde79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1748ded3c564cbfbea7b2cc0ba4c556": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a51cb3b001064f19a0c21ddb15f73edb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a68f7ee837c943f3b99db2383ea38b9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a937a1f89b2c440f86140635f2e1952a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02ff6b9119c446e1ac4886ecc8c3ff43",
              "IPY_MODEL_9b591f42289348428c0b464c3d7d78cf",
              "IPY_MODEL_63c7878bbeac47579624398f13aa1648"
            ],
            "layout": "IPY_MODEL_e75ea20b21dd413e940f474493102753"
          }
        },
        "b06bad84d31a4ee1a6a47ccbfb0e8056": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b36f5bd4466b4847824e935e021b672f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b63241c4d9e5435fb2326465c1276f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b36f5bd4466b4847824e935e021b672f",
            "placeholder": "​",
            "style": "IPY_MODEL_0a48bf6a90dc499e947f7a2b66b106b0",
            "value": " 435M/435M [00:02&lt;00:00, 137MB/s]"
          }
        },
        "c48d05ca768b46899aa407f362a107ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c819c93ba5b74e599d32b50d71c627b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf8d8504f7f74539ae04b9a4f41d0f43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddadfe2620544ee2b7fad3d9e4079792": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d04b242e92a4eb0b533580e96d067db",
            "max": 167,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b2f8cb80a9141798ad48fadf1275bba",
            "value": 167
          }
        },
        "e75ea20b21dd413e940f474493102753": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaa6c5e61444404f8385d6e559462ed9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ede0e0d736f348d3bc0c8cf9c4ea7826": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef8240c370e64c6daf23ae3c4f340671": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_995c4c483b264d5eb9e908c8f5131174",
              "IPY_MODEL_f550683d6ebe47268585cb88445df2c9",
              "IPY_MODEL_b63241c4d9e5435fb2326465c1276f4e"
            ],
            "layout": "IPY_MODEL_8bb5045e0aac42ca9b81daf81b7a024b"
          }
        },
        "f1da2bed9775454cbca6eb4bbb6fec89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f428b58f2e8941ee94070dbd0c4f5540": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1189ae39f0a546e9a521222717aaef56",
            "placeholder": "​",
            "style": "IPY_MODEL_69d92ab1eb9345899bc066c4378b0b69",
            "value": " 167/167 [00:00&lt;00:00, 2.08kB/s]"
          }
        },
        "f550683d6ebe47268585cb88445df2c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ede0e0d736f348d3bc0c8cf9c4ea7826",
            "max": 435234485,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9196423dcb2d43ed8b873845f9a71291",
            "value": 435234485
          }
        },
        "f6a59d90a6c445529b6f0269e7243398": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73a32eb6873a4e7a9bd934a7c1481dd9",
            "placeholder": "​",
            "style": "IPY_MODEL_6bf52c28785b46a5b1450ee7480d51aa",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "f6d208cb0cdd462ea1f048b87e5c8b96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7a45a3c3de349ce8a48c2e8528f2e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65447f68a7814771bd583275aa3c952f",
              "IPY_MODEL_ddadfe2620544ee2b7fad3d9e4079792",
              "IPY_MODEL_f428b58f2e8941ee94070dbd0c4f5540"
            ],
            "layout": "IPY_MODEL_b06bad84d31a4ee1a6a47ccbfb0e8056"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
